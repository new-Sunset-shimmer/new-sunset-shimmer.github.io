[{"content":" Full Title : DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs\nLink : 2406.01721 (arxiv.org)\nRelated Link : SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (arxiv.org)\nReleased Date : Mon, 3 Jun 2024\nReview Date : Mon, 16 Dec 2024\nZigZag matrix for mitigate outlier to activation channels\nMotivation outliers are huge problem for model quantization because it make quantized matrix more sparse and make quantization denotation unstable. Smoothequant is proposed method that smooth outlier in activation and trade-off between wieght matrix. but in figure 1 smooth quant can’t elimate massive activation also in weight new outlier appear. In this paper authors propose smoothing more than smoothequant also trade-off between all blocks\nResearch questions Can we make weight and outlier more smoother?\nMethodology The proposed DuQuant Method\nIn this paper use method of smoothquant. first they smoothing all tensor. but mentioned above this method can’t smooth all dimention especially massive actiavation The Rotation Transformation\nAfter smoothing need to rotation for more flattenness in order to they use random orthonagy matrix R(greedy searched). E is swapping matrix that change column of first index and outlier indeces. Also adding extra randomness they use random orthogonal matrix Q’. R1 is meaning first block, find all R for all block But it is hard to find all R efficiently so they find Cin X Cin once The Permutation Transformation\nEven if we flatten the activation is hard to quantize because of unclear distribution. in one block many outliers are exist. for prevelenting it we need to permutate blocks to reduce variance of median of all blocks. it means we ensure distribute outliers to other block The Overall DuQuant Method\nOverall authors smooth 2 steps\nMain result apply per-token activation quantization and per-channel weight quantization\nPersonal thought little bit messy method but cleaver ","permalink":"https://new-sunset-shimmer.github.io/DuQuant/","summary":"ZigZag matrix for mitigate outlier to activation channels","title":"DuQuant"},{"content":" Full Title : OneBit: Towards Extremely Low-bit Large Language Models\nLink : 2402.11295 (arxiv.org)\nRelated Link : https://github.com/xuyuzhuang11/OneBit, 2310.11453 (arxiv.org)\nReleased Date : Sat, 17 Feb 2024\nReview Date : Fri Dec 13 2024\nmake one-bit model more cheaper way\nMotivation To making model more faster and relax memory-bound 1-bit is effective way to quantize model. But most of the research quantize the model to 2-bit. In this paper quantize model to 1 bit but NOT TRAINING FROM SCRATCH. Research questions Can we quantize 1 bit model Methodology Background General quantization work under equation 1(RTN). But in 1 bit zero-point, scale parameter is useless\nFor quantize in 1-bit most recent work propose equation 2. But in that paper model can’t quantize from pre-trained model, we must train from scratch\n1-bit Linear Layer Architecture From quantize existing model authors propose equation 3. g and h are FP16 parameter. Even if they are extra parameter in FP16, they are in low bit also negligible. when author quantize 4096x4096 matrix total bit is 1.0073\nSign-Value-Independent Decomposition For future training authors initialize weight for efficient training. SVID(SINGULAR VECTOR INDEPENDENT DECOMPOSITION) is Decompose W to W_sign ⊙ W_value [W_sign = Sign(W), W_value = |W|]. We can decompose W to rank-1 approximation and can get equation 4\nIf we denote h and g as b, a moreover W_sing as W_+-1 we can get equation 5. this equation also dont need a restore original weight from Wsing and ab because x is can compute in decomposed matrices.\nthis Initialization of equation 3 is most near to the original linear equation(if we also consider a sign value).\nKnowledge Transfer In this paper transfer teacher model(Full precision model) to student model(1 bit model using equation 3). They use cross entropy loss but not in LM. They train g, h, and W in equation W\nIn equation 7 c is number of class in model and n is total sample(Tokens). Log is wrap student model.\nThey also use hidden state to Transfer knowlegde. n_l is layers and n_s training sample Hence the final objective function can be formulated as equation 9, alpha is hyper-parameter that balances the importance of the CE and MSE. in this paper use 1.0 Zero point quantization? Zero point, ensuring that the quantized representation can handle negative values or values centered around zero.\nNMF? Non-negative matrix factorization(NMF)\nEnsure that decomposed matrices have no negative values(also non-negative matrix approximation).\n[NMF — scikit-learn 1.5.2 documentation]\nFind two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) whose product approximates the non-negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\nL-2 norm L-2 norm is also known as Euclidean norm can measure distance. in using in MSE they are measure distance between two matrix and minimize it. Additionally, They are can use as penalty in loss function for prevalent overfitting and encourage smaller vector. from optimization aspect it tell us gradient size that in step.\nMain result Weight only, use NMF is scikit-learn, use same method for train data from LLM-QAT(2048 token) start from . 50 epoch, Adam, cosine scheduler. GPU is A100(!!!!). Author use Post-LayerNorm for stable qat because floating-point overflow during the QAT process Problem Solving Ability Efficiency T-L for each sign matrix Personal thought Can we train without W in equation 3 ?\nCan we BIT PTQ ?\n","permalink":"https://new-sunset-shimmer.github.io/OneBit/","summary":"make one-bit model more cheaper way","title":"OneBit"},{"content":"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\nDyLoRA라는 논문을 기반으로 작성됨. DyLoRA는 기존에 고정된 rank는 모든 task를 optimal하게 학습하지 못한다는 단점이 존재한다고 비난. Dynamic Lora rank를 즉 rank를 학습중에 의 분포도를 기반으로([r_min, r_max]에서 확률적으로) 선택하여 r_max로 만들어진 low-rank에서 학습한다. QDyLoRA는 위에 방법론에 QLoRA를 적용시킨 논문이다. 그렇기에 QDyLoRA에서는 QLoRA와 성능 비교를 한다.\nMitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization\nper-tensor level QAT다. clipping point인 c-,c+를 학습한다. 양자화 함수는 미분이 아니기에 미분알고리즘 또한 제안했다. 기존 outlier들은 첫 Layer에 생기고 Layer를 거칠수록 그 비율은 줄어든다. 또한 outlier channel은 일반 channel과 다르게 expected value가 0에 존재하지 않는다. 그렇기에 shift까지 학습하는 omniquant가 성능이 좋다고 언급한다. 양자화된 activation에 추가로 weight도 양자화시 ppl이 비약적으로 상승할수있다. 이는 weight를 학습하지 않은건 물론이고 QAT시 Activation의 outlier가 가중치에도 나타나게 된다. 그렇기에 kurtosis로 정규화하여 해당 outlier들을 줄이며 학습을한다(Cross entropy loss에 정규화된 값들의 합을 lambda로 곱하여 더함) kurtosis : 해당 매트릭스가 얼마나 heavy-tailed인지를 확인한다. → Kurtosis는 정규화시 해당 매트릭스의 mean, variance를 활용한다(epsilon도 값을 안정화시키기 위해 들어감). 논문에서는 이를 매트릭스가 얼마나 outlier를 심하게 가졌는지를 확인 할수있다고 언급한다. 추가로 outlier는 Block사이에 존재하는 residual을 기준으로 전파된다는 걸 확인했다. attention input은 outlier가 심하게 존재하지만 FFN input(MLP Proj)에는 존재하지 않는다\n[2402.14866] APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models (arxiv.org)\n양자화시 Attention layer에대한 미분으로 hessian matrix을 구한다. 단순히 linear한 정보로 구하는 GPTQ은 non linear한 정보를 구하지 않기에 문제가 있다. 덤으로 앞서 언급한 Hessian matrix는 민감한 정보를 담기에 acsending sort할시 중요도로 같이 sorting된다. 해당 논문에서는 top %로 4bit 아래의 남은 %는 2bit 양자화 했다.\nIntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact\n모델이 양자화시 (추가)성능저하가 일어나는 이유를 Attention sink에서 언급한 특정 위치에 나타나는 Huge magnitude들의 값이 바뀌면서 일어난다고 지적. 해당 Attention sink가 일어나는 token들을 Pivot Token이라고 명칭. Attention sink에서 언급한 index위치는 무시하고 Initial Token만 FP16으로 가지고 나머지는 양자화. 보통 Attention sink는 [BOS]에 일어나기에 해당 토큰만 FP16으로 보존. 또한 Supervised Finetuned model은 (개인적인 의견으로는 Instruction tuned같다) system prompt가 있을시 성능이 오르기에 해당 토큰도 보존. Attention 시 해당 토큰에 해당 되는 KV값들만 Full precision model에서 사전에 가져와 저장한다. 또한 해당 KV값은 추가 학습 파라미터가 될수도 있다.(토큰이 적기에 학습 비용이 적음). Pivot token이 많으면 많을수록 모델 성능이 좋아진다.\nDynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n비고 : Round(sigmoid(K))로 이전 KV CACHE에 더할지 새로 저장할지 정함\nCOMPRESSING LLMS: THE TRUTH IS RARELY PURE AND NEVER SIMPLE\n비고 : 경량화 방법론들의 실험. Pruning은 30대가 적절하고, quantizaion의 성능은 의외로 높았다.\n[2406.12016] Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization (arxiv.org)\n비고: Calibrated dataset에서 greedy하게 새로운 token들을 prefix 토큰에 추가한다. 추가된 토큰이 젠체 손실함수를 내리지 못하는 순간까지 토큰을 prefix에 추가한다. 이후 prefix를 미세조정한다. Lcrossentropy + lamba Lq. Lq는 양자화된 모델의 loss이며 L(tokens | Prefix_tokens)\n","permalink":"https://new-sunset-shimmer.github.io/extra/","summary":"리뷰할 정도가 아닌 논문들을 간단 요약","title":"1"},{"content":"Atom: Low-Bit Quantization for Efficient and Accurate LLM Serving (arxiv.org)\nATOM: LOW-BIT QUANTIZATION FOR EFFICIENT AND ACCURATE LLM SERVING\nReleased Date : 2023 10 23\nReview Date : 2024 10 20\n보다 나은 양자화\nMotivation LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 이를 해결하기 위해서 양자화라는 기술이 연구가 되고있지만 성능을 크게 떨어트린다. 현재 Atom은 해당 문제를 성능을 보존하면서도 빠른 LLM을 위한 보다 나은 양자화 기술을 서술한다. Research questions 성능을 보존하면서 양자화가 가능한가? Methodology 대부분의 양자화의 성능 저하의 원인으로 outlier가 뽑히고 있다. 평균보다 훨씬높은 값을 가진 outlier 차원은 양자화시 사라져 원본 모델의 성능을 저하시킨다. 그렇기에 outlier 차원만 따로 양자화 시키지 않고 FP16bit 그대로 가져하는 방식이 제안 되기도 했다. 해당 논문의 연구에 따르면 outlier를 INT8bit로만 양자화 해도 충분한 성능을 보여준다는 사실을 발견했다. 가중치, 활성화 함수를 4bit 양자화, outlier는 8bit로 양자화를 한 MIXED-Precision은 계산 효율이 높지가 않다. 해당 문제를 해결하기 위해서 outlier들의 index 및 가중치의 index를 바꾸는 reorder 방식을 사용했다. 원래는 reorder를 online로 해야하지만 계산효율을 어기는 행위 이기에 offline로 reoerder했다. Mixed-precision 계산을 효율하기 하기위해서 Fused GEMM이라는 함수를 구현했다. outlier와 일반 index들을 각각 계산되어 unquantize되고 더해진다음 다시 Quantization을 한다. 또한 KV-cache도 quantazion 된 채로 존재한다. Main result 128로 랜덤하게 samplig된 wikitext2를 기반으로 outlier를 계산하였다. Personal thought 사전에 outlier를 계산하는 건 좋은 시도라고 생각한다. 동시에 따로 계산하고 마지막에만 합치는 것도 좋은 생각이라고 생각한다. ","permalink":"https://new-sunset-shimmer.github.io/ATOM/","summary":"보다 나은 양자화","title":"ATOM"},{"content":"2309.17453 (arxiv.org)\nFull Title : EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS\nReleased Date : 2024 04 07\nReview Date : 2024 10 21\nAttention sink를 해결하여 window attention 방식을 효율화\nMotivation LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 계산 비용을 낮추기 위해 흔히 사용되는 KV cache 방법론은 효과적이다. 그럼에도 불구하고 KV cache 큰 메모리 비용을 잡아 먹는 단점이 존재해 특정 토큰을 잘라 내지만 성능을 많이 깍아 먹는다. 이번 논문에서는 특정 토큰을 잘라 사용 하면서도 성능을 보존하면서 계산 효율을 올리는 방법론을 제시한다. long context에도 많이 쓰인다. Research questions 효율적이고 빠른 LLM 서비스를 위한 계산효율을 어떻게 늘릴까? Methodology 저자들은 Attention sink이라는 어텐션 Layer에서 sequence의 오직 첫번째 token에 sementic에 상관없이 무조건 과집중하는 현상이 발견했다. 저자들은 이러한 현상이 발생하는 이유를 2가지의 원인이 존재한다고 설명했다\nsoftmax 함수는 절대로 0이라는 값이 나오지 않으며 모든 token에 일정 비율 학습이 진행된다. - Auto-regressive 모델의 특성상 첫번째 token이 중복으로 학습된다. - 위의 문제들을 근거하여 위해 저자들은 첫번째 token을 살리면서 window attention을 진행하는 방식으로 문제를 해결했다 Sink token이라는 token을 학습시 맨 앞에 넣어 오직 하나의 token에만 집중할수있게 만들어 모델을 pretraining 시켰다. 모델은 Pythia-160M Append Personal thought slind window을 StreamingLLM에 적용시키면 어떻게 되는가? ","permalink":"https://new-sunset-shimmer.github.io/ATTENTION_SINKS/","summary":"Attention sink를 해결하여 window attention 방식을 효율화","title":"ATTENTION SINKS"},{"content":" Full Title : AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models\nLink : 2311.01305 (arxiv.org)\nRelated Link : SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (arxiv.org),\nReleased Date : Thu, 2 Nov 2023\nReview Date : Tue, 10 Dec 2024\nActivation-Weight Equalization + Bias Correction\nMotivation From recent(in 2023) research they are propose mitigate complexity to another. In example, Smoothquant they are mitigate activation quantize complexity to weight complexity. Activation got huge magnitude values named outliers(some paper name it as salient value) make quantization harder. Also awq mitigate complexity to another in order to quantize in extremely low-bit. In this paper mitigate activation quantize complexity, bias error to weight by per channel also ensuring robustness of the model. Research questions can we mitigate complexity also guarantee robustness Methodology Quantization with Activation-Weight Equalization(AWE)\nr is represnet equalized i-th cannel, R denote range of the equalized tensor, specifiacally, it is the maximum value minus the minimum value for all channel.\nIn the end pi is equalized i-th channel by whole tensor\n- Authors aim to find s such that the total precision(pi) per channel is maximized. - equantion 5 is can derive to equation 6. R is can be maxj(1/sj * rj) that is maximum value of the equalized tensor. r is i-th channel without equalization. - Authors can sum-out r(this value not very important to find s). Authors only use argmaxs that can be denote argmins if we change divinder to divinded. - uniqueness for robustness(simplify) we set equation 9 manually. because if we thins argmaxj and argmaxk is not equal there is numerous solutions for equation 8 - To further address the issue of diversity in the solutions for s, we directly assume r(What) = r(xhat)(equalized channels are equivalent), Finally we can say si make ri(w) and ri(x) equal. so we can express it as equation 10 Quantization Bias Correction(BC)\nwe can get Block’s bias error(how much expected value moved) by equation 11. N is represent the number of input data used to count the quantization error - epsilon is sub of quantized weight with original weight - by derivation authors can get equation 15. We can obtain epsilon by precompute for activation quantization. Main result Accuracy of AWEQ average accuracy of table 1 8A8W ablation experiments on AWE, BC impact on model Personal thought low explain on equation derive ","permalink":"https://new-sunset-shimmer.github.io/AWEQ/","summary":"Activation-Weight Equalization + Bias Correction","title":"AWEQ"},{"content":" Full Title : AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION\nLink : arxiv.org/pdf/2306.00978\nReleased Date : (MLSys 2024) Review Date : 2024 10 26\nQuantize된 가중치에 스케일링을 줘서 outlier를 따라하기\nMotivation PTQ는 따로 학습이 필요없이 모델의 사이즈와 계산량을 줄일수있는 장점이있다. 하지만 “Outlier”라는 중요한 벡터에 인해서 방법론에 인한 성능저하에 인해서 과연 효과적인지 의문을 가질수있다. 이번 논문은 그런 Outlier들을 quantization한뒤에 따라하는 방식을 제안했다. Research questions Local에서 돌릴수있는 모델을 만들수있을까? Methodology 저자들은 가중치중에서 거의 0.1%에서 1%프로만이 최종 결과에 영향을 미친다는걸 확인했다. 입력의 문맥에 맞는 소수의 가중치만 FP16으로 보존하면 성능에 영향을 미치지않는 양자화가 가능하다고 저자들은 생각한다. 하지만 그럴경우 계산속도는 여전히 비효율적이다. 저자들은 처음에 가중치가 양저화 됐을 때의 오류를 찾는 작업을 했다. s가 가장 최적으로 Perplexity를 보존하는 지점이 존재한다. - 일반 양자화 세모’은 s를 추가한 Quantization scalar 값이다. - Scale(s)를 추가한 양자화 - 각 양자화식의 오류 차이를 찾는 식 하지만 s를 직접찾는건 비 효율적인이다 그렇기에 저자들은 s를 찾는 식을 제안했다. 위의식으로는 직접적인 Gradient를 구할수 없기에 (Q 함수는 미분불가능) 분완전하다. 그렇기에 아래에 식을 적용함으로서 좀더 안전하게 학습한다. x는 활성화 함수를 지난 값이다. alpha를 통해서 [0,1] 얼마많큼의 가중치를 가져갈건지 정한다. Main result 해당논문은 가중치 그룹 양자화 방법론이다 . 그룹 사이즈는 128다. Calibratoin dataset은 Pile이다. 기본 양자화 방법은 RTN이다. Personal thought Activition도 양자화시 성능이 올라갈까? ","permalink":"https://new-sunset-shimmer.github.io/AWQ/","summary":"Quantize된 가중치에 스케일링을 줘서 outlier를 따라하기","title":"AWQ"},{"content":" Full Title : DUOATTENTION: EFFICIENT LONG-CONTEXT LLM INFERENCE WITH RETRIEVAL AND STREAMING HEADS\nLink : 2405.16406 (arxiv.org)\nRelated Link : 2404.15574 (arxiv.org), 2309.17453 (arxiv.org)\nReleased Date : 2024 10 14\nReview Date : 2024 11 03\nAttention sink + Retrieval attention을 이용한 Token pruning\nMotivation 현재 많은 LLM은 O(n^2d) 복잡도 문제를 겪고있다. 해당 문제를 해결하기 위해서 Attention head의 Key하고 Value를 재사용하는 방법을 사용하고있지만 cache 사이즈가 크다는 문제를 겪고있다. 이번 논문에서는 Attention Head 2를 써서 해당 문제를 해결할려고 한다. Research questions 효과적인 Token Pruning 방법은 존재하는가?\n모든 Head가 중요하지 않다면 해당 Head를 다른 방식으로 써도 되지 않을까? 2404.15574 (arxiv.org)\nMethodology Retrieval Head Mechanistically Explains Long-Context Factuality에서 가중치, activation, layer처럼 모든 head가 중요하지 않다는걸 실험으로 확인했다.\n저의 생각으로 저자들은 해당 결과를 기준으로 아이디어를 Token Prunning을하여 KV Cache를 줄이는 방식으로 발전시켰다.\n모든 head가 중요하지 않다는건 몇몇 head들만 Full attention으로 쓰고 나머지는 Approximate attention을 활용할수있다. 저자들은 해당 head를 2309.17453 (arxiv.org)에서 방법론을 가져왔으며 이를 Streaming head라고 명명했다.\n저자들은 2309.17453 (arxiv.org)에서 제안한 Attention sink + window attention을 Streaming Attetion명명했다. 실험적으로 Streaming head는 Streaming Attention의 영향을 즉 Token Pruning을 해도 성능에 큰 영향이 안간다. 이는 Streaming Head를 써도 큰 문제가 없음을 의미한다. 저자들은 학습 파라미터 Alpha를 둬서 Full Attention x alpha, Streaming Attention x (1 - alpha)를 둬서 학습한다. 이휴 서비스(Deployment)에 Alpha를 Threshold Tau로 어느 Head를 쓸지 정한다. 정의 : retrieval head : 최근 토큰과 attention sinks만으로 사용했을 때 모델 성능에 크게 영향을 끼치는 head. 저자들은 단순한 Natural language만으로 위의 학습이 어렵다고 판단했다. Natural language만으로는 head의 Retrieval 성능을 확인하기 어렵기에 실제 무수이 많은 Token에서 정답 일정한 길이에 Passkey만 찾는 dataset을 만들었다. 실제 학습단계에서는 attention score가 위의 식처럼 계산이 된다. M casual은 일반 attention 마스크이며 M streaming은 Λ형태의 마스크다.(오직 Attention sink와 recent token들만 보이게 하는 마스크). alpha는 0,1사의 값으로 초기화된다.\n저자들은 Loss function 2개를 만들었다. L_distill, L_reg. L_distill은 passkey의 token들의 attention값을 계산하여 L2 Norm을 한다. L_reg는 총 alpha값을 계산한다. 개인적인 생각으로 L_distill은 H_full과 H_mixed의 Attention값을 0으로 만드는 방향으로 학습된다고 생각한다. 그렇기에 L2 norm을 사용했고 따로 Lr을 적용시키지 않은것이다. L_reg은 H_Full의 중요도를 계산하는 의도로 보인다. 이번 논문의 L_reg을 만드는데 영감을 준 cnn filter pruning에서 채널의 중요도를 계산한다. 여기서는 H Full의 중요도를 적게 만드는 방향으로 학습한다. 최종 loss는 Retrieval 해드를 적게쓰면서도 방향으로 학습하면서 L_dsitill을 완충제로 사용하여 모든 해드가 Streaming이 되지 않도록한다. 해당 논문에서는 FlashAttention-2를 사용한다. 해당 방법론특성상 Context길이가 짧으면 짧을수록 속도가 일반 Attention보다 빠르게 올라간다. 그렇기에 Context를 일정한 길이의 길이로 나눈다. 이를 Chunked pre-filling이다. 이를 통해서 복잡도가 O(L^2)에서 O(LK)로 줄어든다. 도한 메모리 복잡도가 O(L)에서 O(K)로 줄어든다. Main result needle in stack 성능 long bench의 성능 short bench에서의 성능 Chunked prefill이 속도와 메모리 측면에서 굉장히 효율적이다. 양자화와 같이 사용시 속도가 더 빨라진다. Personal thought Loss function을 보면 결국 최대한 Full attention을 사용하지 않는 방향으로 학습이된다. 비록 L_distill로 방충망을 만들었어도 결국 Retrieval head가 필요한 시점에서도 Streaming head를 쓰게 되지 않나? ","permalink":"https://new-sunset-shimmer.github.io/DUOATTENTION/","summary":"Attention sink + Retrieval attention을 이용한 Token pruning","title":"DUOATTENTION"},{"content":" Full Title : FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization\nLink : 2402.17985 (arxiv.org)\nRelated Link : SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (arxiv.org)\nReleased Date : Wed, 28 Feb 2024\nReview Date : Wed, 11 Dec 2024\nSmooth and Flatten for better speed and accuracy\nMotivation Mixed-precision quantization is not very efficient, By Adding batch to input it make model to Compute bound in Mixed-Precision weight. In this paper quantize model as Mixed-precision but in low-bit(8,4) and make outlier flatten to easy to quantize\nResearch questions can we make mixed-precision more effective\nMethodology This paper uses smoothing and flattening techniques. The smoothing approach comes from SmoothQuant. The authors criticize SmoothQuant for being ineffective when outliers are extremely large. Authors propose Flatten that split outlier to new channel also Repeat corresponding channel of weight to flattening a outlier. in the end the Quantize model in low-bit mixed-precision.\n(a) is showing how to flattening. (b) is visualizing after a (Flatten+Repeat).\nFlattening the Tensor\nAuthor splitting outlier by treshold T. Ej is count of extended channels, Total count of extended channels are denote as C_extended\nWe Flatten(extend outlier by number of E) and original xij is mod by T. Wj is repeat by count of extended channel\nAchieving High-precision\nAuthor smoothing X and W in order to reducing total number of count. Authors find S by expected value(median) and distribution on channel\nSelection of the truncation threshold\nThey can find threshold value for avoid increasing channels hugely. Q1 is lower quartile Q2 is upper quartile that divide data into lowest and highest 25% respectively. IQR is interquartile range defined as Q3 - Q1. and result we can find Threashold value that normalized by all tensor.\nQuantize some layers to INT4.\nAuthors goal is reduce compute speed. Int 4 is more hardware friendly. Authors find LAYER performance in 4 bits using KL divergence of P(Original data distribution) and Quantized tensor distribution(Q) with comparing threshold γ.\nMain result per-tensor quantization, OPT model, α is set to 0.5, β is set to 1.3, γ as 1.86\nPersonal thought this paper focus on speed of when batch is huge\n","permalink":"https://new-sunset-shimmer.github.io/FlattenQuant/","summary":"Smooth and Flatten for better speed and accuracy","title":"FlattenQuant"},{"content":"https://arxiv.org/pdf/2402.04617\nInfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory\nReleased Date : 2024 02 07\nReview Date : 2024 10 04\ntoken을 split하여 KV cache를 줄임\nMotivation KV Cache는 효과적인 attention을 위한 방법이다. 하지만 Long Tokens의 경우 많은 Cache가 필요하기에 메모리를 많이 차지하는 문제가 존재한다. 이번논문에서는 해당 문제를 Token Split으로 해결할려고 한다. Methodology Token들을 특정 사이즈로 짜른다. 초기 토근(0,l), 중간토근(l,nl), Local token(nl,nl+l)을 이용하여 Evicted Token을 정한다. Evicted tokens는 Context Memory에 저장되며 실제 Gpu에는 Representative Tokens(r_m)만 저장된다. Current token과 r_m을 내적을 구하여 가장 높은 점수의 k를 가져와 Initial, Evicted, Local Token들로 attention을 수행한다. Main result ","permalink":"https://new-sunset-shimmer.github.io/InfLLM/","summary":"token을 split하여 KV cache를 줄임","title":"InfLLM"},{"content":" Full Title : KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\nLink : KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization (arxiv.org)\nReleased Date : 2024 06 04\nReview Date : 2024 10 26\nLong context에서의 Quantazation\nMotivation LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 이번 논문에서는 KV cache의 K를 RoPE(Rotary Position Embedding) non-uniform quantazaiton을 하여 긴 context에도 효율적으로 계산할수있게 한다. 또한 Research questions Long context에서의 양자화가 가능할까? Methodology 해당 논문에서는 총 4개의 방법론을 적용시키고 있다.\nPer-Channel Key Quantazation : Key matrix를 Channel별로 양자화시 좋은 성능이 나온다는걸 확인했다(quary는 아님). 그 이유를 Key는 outlier가 잘 발생하지 않고 한 채널은 스케일링 값과 0값을 공유해서 그렇다고 저자들은 서술했다.\nPre-RoPE Key Quantization : Rope를 적용시킨채 Key를 양자화할 경우 한 매트릭스에 위치 정보 및 여러 정보가 섞이기에 분포도가 바뀔수있다. 이는 양자화를 어렵게 만들기에 RoPE를 적용시키기전에 양자화를 진행한다.\nNon-Uniform Quantazation : 등 양자화는 KV 캐시 양자화에 비효율적인데, 이는 Query와 Key 활성화 값이 비균등하게 분포되어 있기 때문입니다. 또한, KV 캐시 로딩은 배치 크기나 시퀀스 길이와 관계없이 메모리 대역폭에 의해 제한되므로, 비균등 양자화에서 발생하는 복원 오버헤드는 문제가 되지 않습니다(추가되는 연산이 지연을 초래하지 않음). 따라서 KV 캐시 양자화에는 비균등 양자화 방법을 활용하는 것이 바람직합니다. 비균등 데이터타입을 계산해야하는데 이는 온라인으로 하기 힘들기에 사전에 calibration dataset으로 offline에서 계산합니다.\n- Per-Vector Dense-and-Sparse Quantization : 벡터마다 다른 값을 의미를 가지고있기에 해당 벡터의 outlier가 꼭 다른 Layer에서 outlier가 된다는 보장이 없다. 그렇기에 벡터별로 다른 outlier Threshold를 둬 보다 동적으로 양자화를 한다. 이들은 16FP 그대로 저장된다. - Attention Sink-Aware Quantization : Sink-Aware는 양자화에도 영향을 미친다는걸 저자들은 알아냈다. 그렇기에 첫번째 token만 16FP로 나둔다. 앞서 언급한 Non-Uniform Quantazation에서 첫번째 Token은 없앤다. Main result Personal thought Activation 도 같이 quantazation이 가능한가?\n왜 FLashattention, pageattention을 안 썼는가?\n만약 context길이 늘어나면 어떨까?\n","permalink":"https://new-sunset-shimmer.github.io/KVQuant/","summary":"Long context에서의 Quantization","title":"KVQuant"},{"content":" Full Title : LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models\nLink : 2310.08659 (arxiv.org)\nRelated Link : https://github.com/yxli2123/LoftQ\nReleased Date : November 29, 2023\nReview Date : Dec 2, 2024\nQuantization aware initializing\nMotivation There is gap between Full fine-tuning and Quantization plus Lora fine-tuning. In this paper propose framework of how to initialize low-rank for quantized model fine-tuning. this paper challenging 2-bit and 2/4bit mixed precision Research questions How to Fine-tuning quantized model with PEFT Methodology Authors promising initialization for LoRA fine-tuning\nAuthors minimizing equation 6. Discrepancy between original weight also between wight and quantized weight.\nAuthors solve equation 6 solve by singular value decomposition. begin with A_0 and B_0 to 0. At-1 is last time step. qN is N bit quantization Qt From equation 7 we can use it in denotion, Rt = W - Qt and we can obtain Initialized by Finetuning aware quantaziation. total algorithm is here Main result fine-tuned llama vs pre-trained llama initialization discrepancy between the QLoRA and LoftQ Encoder-only Model: DeBERTaV3 Encoder-Decoder Model: BART Decoder-only Model: LLAMA-2 Personal thought initialization is important in PEFT\nwhy we subtract Low-rank to Weight?\n","permalink":"https://new-sunset-shimmer.github.io/LoftQ/","summary":"Quantization aware initializing","title":"LoftQ"},{"content":"2309.12307 (arxiv.org)\nLONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODEL\nReleased Date : ICLR 2024\nReview Date : 2024 10 20\n긴 context를 위한 모델 만들기\nMotivation LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 또한 사전학습에서 정한 최대 길이 이상의 context를 입력으로 받을 경우 제대로 된 성능을 못보여주는 단점도 존재한다. 해당 문제를 해결하기 위해서는 context 길이를 늘려서 continious training을 해야하지만 앞서 언급한 단점에 인해서 쉽지 않다. 본 논문에서는 긴 context길이를 위해서 효율적인 학습 방법을 제안한다. Research questions 긴 context를 위한 효율적인 학습 방법이 존재하는가? Methodology 해당 논문에서는 효율적인 학습을 위해 기존에 쓰이는 lora(LOw Rank Adaptation)을 채용하는 동시에 오직 Lora만으로 학습시 성능이 나오지 않으며 Embedding 차원을 따로 학습 시키지 않으며 결국 context 길이가 제한 되기에 Embedding 차원 그리고 새로운 사이즈를 위해 normalize layer도 학습한다. 어텐션 Layer의 비효율성을 제거 하기 위해서 해당 논문에서는 Shifted Sparse Attention을 제안했다. Shifted Sparse Attention(s^2-Attn) : Context를 Group으로 묶고 head들을 나눈다. 어느 한 head 그룹은 Context Group의 절반 사이즈 만큼 token의 index를 바꾼다. 이로 인해서 Context를 나눠도 head끼리 정보 교환이 이루워져 성능이 올라간다. Main result 모델은 Llama2 모델을 사전학습을 진행했다. 7B은 100K context size, 13B은 65536, 10B은 32768로 설정했으며 LR은 7B,13B에 2 x 10^-5로 설정해줬다. 데이터셋은 Redpajama로 학습 평가는 PG19, cleaned Arxiv Math proof-pile dataset을 섰다. 또한 embedding Layer를 늘리기 위한 쓴 Position Interpolation방법론에서 평가를 진행한 데이터셋들도 재사용했다. 오직 LoRA만 쓰는건 비효율적이라는 것을 보여주는 table Retrieval 성능도 평가하였다. Personal thought S^2 Attn은 마지막에 contcate 될떄만 정보를 교환하는가.\nembedding을 다른 방식으로 바꾸면 어떻게 되는가. 예를 들어 absolute position encoding으로 바꾸면 속도가 더 올라가지 않을까?\n","permalink":"https://new-sunset-shimmer.github.io/LONGLORA/","summary":"긴 context를 위한 모델 만들기","title":"LONGLORA"},{"content":" Full Title : LQ-LORA: LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION FOR EFFICIENT LANGUAGE MODEL FINETUNING\nLink : 2311.12023 (arxiv.org)\nRelated Link : https://github.com/HanGuo97/lq-lora, 2305.14314 (arxiv.org), 1009.5055 (arxiv.org), 1010.2955 (arxiv.org), 0912.3599 (arxiv.org), 2110.05649 (arxiv.org)\nReleased Date : Mon, 20 Nov 2023\nReview Date : Thu Nov 28 2024\nQuantize Lora for efficient model finetuning\nMotivation There is plenty research on adding Low-rank adaptation at quantized weight for efficiency and speed. But Quantized q(W) + AB isn’t equal orignal W. In this paper update low-rank updates quantized W(q(W)) with quantization error. Inoder to do that, Authors decompose W into Q + L1 L2, train L1 L2 as Low-rank adaptation Research questions How to update quantized weight by Lora Methodology NormalFloat (NF): NF is for Quantile Quantization\nquantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. - Quantile Quantization : As mentioned above, quantile is divide distribution range into continuous equal intervals. Quantile quantization is quantize value to divided quantiles(also quantization bin). in one phrase we quantize by distribution. Example if NF4 we generate [-1.0,-0.8667,-0.7333,-0.6,-0.4667,-0.3333,-0.2,-0.0667,0.667,0.2,0.3333,0.4667,0.6,0.7333,0.8667,1.0], and input 0.5 is can be quantized to 0.4667. - But this is very slow, so use faster quantile like SRAM quantiles. Inorder to fast quantiles we need go fix expected values, variance. in QLoRA paper refrence Transformer model’s expected value is 0, make k-bit quantiles between [-1,1]. Authors divide matrix into specific number of blocks. As example d X k matrix into dk/B (B is block size).\nLQ-LORA\nLOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION\nIn LoRA reparamatereizes a pretrained matrix into W + L1 L2. They initializes L1 from a Gaussian, L2 to 0. It is ensure that L1 * L2 = 0 so model can finetune from zero(original distribution). But in quantized weight it is not very effective. Especially in low bit quantization error is ||W - Quantized(W)|| \u0026raquo; 0. Authors propose Low-rank parameters need to be initialized by structure of Weight(w).\nHere is General loss function(1) between W and quantized Q + low-ranks. Q_b is mean NF-quantizable to b-bits. As authors. this optimization problem is similar to the one faced in robust principal components analysis. Authors solve it by approximately via altenating between optimizing L1L2, Q. Drive from equation 1. we can obtain new equation. Q^0 is intialized to 0. Above equation is found by heuristic. By Algorithm 2 we find optimal Q,L1,L2. Because authors use randomized SVD(for better speed improvements). simply check which loss is higher than previous time step’s L1 and L2, Q. Also optimizing RPCA via iteration expriments on loss values on different equetions. - **Robust principal component analysis** - Randomized SVD - MIXED-CONFIGURATION QUANTIZATION VIA AN INTEGER LINEAR PROGRAM For better store authors choose double quantizetion Quantize-NF is model quantization. Qunatize-INT is quantize the quantiles into integers. Quantiles got very small numbers it is almost lossless. - Dynamic quantization configurations. Prior works applying same quantization method to all matrix. All matrices aren’t got same value, same distribution also some of them is much harder than others. In this paper applying different configure to each matrix. - DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD mat Minimizing assignment matrix X that minimizes the Frobenius norm between the matrices before and after low-rank plus quantized decomposition, while respecting a target memory budget. for better minimize quickly authors use API [https://www.gurobi.com](https://www.gurobi.com/) DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD\nFollowing recent works Authors use Fisher information\nFrom calibrate dataset(d numbers) calculate derviation. Fisher information contain sensitivity of data and importance. Generally use Hadamard product. can apply to finding for L1, L2. equation above is np-hard. So authors relax problem to multiplying Column, Row of Fisher information. It is also can solve by standard svd Main result Finetuning of LQ-LoRA Quantized LoRA GLUE with RoBERTa-Large Personal thought Why is not Fisher information general? ","permalink":"https://new-sunset-shimmer.github.io/LQ-LORA/","summary":"Quantize Lora for efficient model finetuning","title":"LQ-LORA"},{"content":"arxiv.org/pdf/2402.17762\nMassive Activations in Large Language Models\nReleased Date : 2024 08 10\nReview Date : 2024 10 21\nMassive activations에 대한 조사\nMotivation 저자들은 LLM에 대한 특이한 형상을 하나 발견하였다. 아주 소수의 활성화 값이 아주 높은 값을 가지는 형상이 발견되었다. 그 소수의 활성화 값은 들어온 context, sementic에 무관하게 특정 위치, 특정 tokens에 나타났다. 이번 논문에서는 해당 형상에 대한 조사를 다룬다. Research questions 왜 Massive activation은 생기는가? Methodology Massive activation?\n저자들은 LLM의 내부의 활성화 값들을 확인하였다. 그중에 평균보다 월등이 높은 값을 가지는 값들 또한 존재한다. 저자들은 이를 Massive activition이라 칭한다. 아래는 모델별 가장 높은 값들을 가지는 활성값들의 값을 나열한것이다.\n어디에 나타나는가?\n저자들은 현재 많이 사용되는 모델들을 비교 분석을 실시하였다.\nLLaMA2-7B : (1415, 2533 ) 차원, 가장 첫 위치의 token, . (period), \\n (newline) 에 생긴다 - LLaMA2-13B : (2100, 4743) 차원, 가장 첫 위치의 token. - Mixtral-8x7B : 2070 3398차원. 가장 첫 위치의 token, and, of 에 생긴다. - 또한 Massive activiation이 생기는 Layer는 대부분 첫번째 그리고 마지막 Layer 뺀 나머지 Layer에 집중되어 있다. - 저자들은 위의 토큰들은 특별한 의미를 내포하고 있지 않거나 혹은 과하게 많이 나타난다는 특성이 존재하기에 Massive attention이 생긴다고 추측했다. Outlier와 다른점은 ?\n저자들은 Massive activition은 scaler 값이고 outlier는 거의 모든 token에 에 나타나는vector값이라고 구분을 지엇다. 또한 outlier가 나타나는 위치에는 Massive activition이생기지 않고, Massive activition이 생기는 위치에는 outlier가 생기지 않는다. Massive activition의 기능은 ?\n저자들은 Massive activition의 값을 평균, zero로 바꿔 성능을 비교했다. 그 결과 Massive activition도 어느정도의 순 기능을 하며 모델이 필요로 하는 값임을 알수있다. Massive activition은 왜 생기는가?\n저자들은 해당 이유를 모델의 특성에 있다고 생각한다. 일단 auto-regressive model의 특성상 첫번째 token이 과하게 중복으로 학습되며 동시에 softmax 함수의 특성상 절대 0은 나오지 않는다. 결국 모델이 Massive activition을 생성할 수 밖에 없는 형태로 학습이 된다는 말로 해석할수있다.\n아래는 Layer 3의 start, period token에 대한 massive 값의 layer normalize 이후의 변화이다. Massive activition의 값은 낮아지지 사라지지 않는다.\n아래는 Massive activition을 방지 하기위해 sink Token, attention Bias를 넣어 실험을 한 그래프다. 3다 성능은 비슷했지만 Massive activition의 값이 달랐다. attention bias Main result 저자들은 결국 아래와 같이 결론을 내렸다 대규모 활성화는 셀프 어텐션(self-attention)과 연결됩니다. 대형 언어 모델(LLM)은 대규모 활성화를 사용하여 매우 적은 수의 토큰에 상당한 주의를 집중시키며, 어텐션 계산에 암묵적인 편향 용어를 주입합니다. 또한, 명시적인 어텐션 편향을 추가하면 대규모 활성화를 제거할 수 있습니다. Personal thought outlier도 일종의 Massive activition이 아닌가?\nself-attention의 노이즈가 존재하 않으면 Massive activation도 존재하지 않는가?\n","permalink":"https://new-sunset-shimmer.github.io/MATT/","summary":"Massive activations에 대한 조사","title":"Massive Activations"},{"content":" Full Title : Norm Tweaking: High-performance Low-bit Quantization of Large Language Models\nLink : 2309.02784 (arxiv.org)\nRelated Link :\nReleased Date :\nReview Date :\nTweak quantized Layernorm to as same as Floating point layernorm distribution\nMotivation Quantized model’s distribution is diffrent from original model. In this paper set a goal that tweaking Layernorm distribution to enhancing quantized model accuracy. Research questions could we improve the performance of the quantized model by simply matching its activation distribution to that of the float model? Methodology Authors define problems as minimizing original model’s weight and Quantization model’s weight distribution(Equation 1). But training on all weight is expensive so authors freeze all weights expect Layernorm weight(W_ln). For fast train and not damage to model(layer norm is very sensitive) on only small iterations. Instead of using already generated calibrated dataset like wikitext using dataset that model generated texts. if we use texts that using own model it can be more general and get rich sementic information. for ensure Generality authors generate from one single token that came from model vocalbury(set a restrict only use top languages that compse most of dataset) Channel-wise Distribution Loss(2). 1) model got outlier that make quantazation harder. 2) strict aligment make over-fitting on calibrated datasets. All over authors make relaxed loss function that learn variable and mean of each channels. all layer got different learning rate for different speed Main result find lr by grid search set at 1e-5, use quantaztion algorithm as GPTQ group 64\nAccuracy on LAMBADA dataset\nNorm tweaking cost Using another quantazition method with norm tweak example of generation for show more accurate performance learning many iterations is very bad to model performance Evalution on LM eval harness Generated data set is make more general quantized model Loss dist is got more accracy Personal thought is quantization learnable? is quantization deriveable? ","permalink":"https://new-sunset-shimmer.github.io/Norm%20Tweaking/","summary":"Tweak quantized Layernorm to as same as Floating point layernorm distribution","title":"Norm Tweaking"},{"content":" Full Title : OMNIQUANT: OMNIDIRECTIONALLY CALIBRATED QUANTIZATION FOR LARGE LANGUAGE MODELS\nLink : 2308.13137 (arxiv.org)\nRelated Link : 2304.09145 (arxiv.org), SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (arxiv.org)\nReleased Date : ICLR 2024\nReview Date : 2024 11 04\nPTQ의 효율성을 가지면서 QAT의 성능을 가지는 양자화 방법론\nMotivation LLM은 놀라운 성능만큼 놀랍도록 큰 계산량과 계산자원, 사이즈를 가지고있다. 그를 위해서 모델을 양자화하는 방법론이 많이 존재한다. 하지만 PTQ를 할시 양자화 속도는 빠르지만 성능은 QAT보다 낮으며 QAT는 그 성능이 높지만 학습이 느리다. 이번 논문에서는 둘의 장점만 살리는 방향으로 연구를 하여 방법론을 제시했다고 생각한다. Research questions QAT의 성능을 달성하면서도 PTQ의 시간 및 데이터 효율성을 유지할 수 있을까? Methodology 이번 논문에서의 최종목표는 학습가능한 파라미터들을 모델에 적용시켜서 양자화에 친숙한 모델로 변환시키는게 최종목표다. 앞서 언급했다싶이 QAT와 PTQ의 장점을 가져가기를 원하기에 둘중 장정과 단점 사이의 어느 선을 정했다는 느낌이 강하다. 위에서 보다싶이 일반적인 모델은 양자화하기 굉장히 불친절하다. 저자들은 이를위해서 학습가능한 Layer인 Weight Clipping(LWC), Equivalent Transformation(LET)을 모델에 적용시켜 학습시킨다. 결론적으로 양자화에 친숙한 모델을 만든다. 저자들은 PTQ 방법론을 학습에 최적으로 만들기 위해서 모델 최적화에서 블록 최적화로 downstreaming을 시켰다. 아래의 수식을 보면 F는 Transformer의 블록이고 Q는 양자화 함수, theta_1, theta_2는 학습가능한 파라미터이다. 결국 양자화된 블록과 일반 블록의 손실을 최소화시키는 theta_1, theta_2를 찾는 Problem Space로 문제를 단순화시켰다. 이를 BLOCK-WISE QUANTIZATION ERROR MINIMIZATION이라 칭했다. LWC(LEARNABLE WEIGHT CLIPPING)은 가중치의 양자화 난이도를 낮추기 위해서 존재한다. 위의 식을 이용하여 최적의 Clipping 지점을 γ , β 를 학습하여 찾는다. 이둘은 0에서 1사이에 무작위 값으로 초기화된다. 저자들은 가중치의 최대값과 최소값을 얼마나 어느정도로 스케일링 하여 clipping시 양자화에 적합한지를 학습시킬 의도로 해당 식을 섰다고 생각한다. 결국 최소한의 양자화 오차를 내는 지점을 학습시킨다고 생각한다. theta_1 = γ , β\nLET(LEARNABLE EQUIVALENT TRANSFORMATION)은 말그대로 학습하는 유사 트랜스포머의 Layer다. T는 원본 Layer고 T’는 LET다. 이둘은 전혀다른 수식을 가지지만 X의 대한 맵핑은 서로 비슷하다. 개인적으로 Equal이 아닌 EQUIVALENT라고 칭한건 해당 학습 space map을 단순화시키기 위한 가정이라고 생각한다.\n결국 T’(X)는 Activition 양자화를 위해서 Outlier를 억제하는 방향으로 학습을 해야한다. 저자들은 이전 연구인 SmoothQuant, Outlier Suppression+에서 영감을 얻어 수식들을 바꿨다.\n- Linear Layer(EQUIVALENT) 일반적인 Linear layer의 식 XW+B를 변환시켰다. δ와 s를 학습시킨다. 이전 연구에서도 확인됐다싶이 매트릭스에 사라지는 어떠한 값을 곱해주면 outlier가 사라지는 현상이 있다. 저자들은 s를 element wise로 x의 divinder, W의 multiple값으로 설정해줬다. 해당 곱해지고 나눠진 S들은 linear 계산시 사라진다. 그렇기에 해당논문에서는 s를 Scale로 부른다. s ∈ R^1xC_in δ은 Shift로 가중치의 분포도를 양자화에 맞게 재조정하는 역할을 한다고 생각한다. 논문에서는 적혀있지 않지만 해당 방법론의 코드상에서는 shift값을 Layer norm의 bias로서 들어간다. 그렇기에 저자들은 이전 Layer norm에 흡수될수있다, 해당 Layer에서 쓰일수있다고 서술했다. δ ∈ R^1xC_in\n이미 학습된 B의 분포도를 새로운 X^tilde 맞춰주는 작업이 필요하다. δ의 형태를 B의 1 x C_out과 맞춰주기 위해서 W와 곱해줬다. 개인적으로 이는 출력 행렬의 차원을 δ로 스케일링하는 것으로 보인다. 단순히 B의 분포도만이 아니라 이전 X - δ로 옮겨진 분포도를 다시 원래 Y로 맞추는 현상이 나타날수도있다. 해당 추측은 정확하지 않다. 저자들은 Outlier supression+에서 영감을 얻어 해당 식을 만들었기에 자세한 설명을 하지 않았다.\n결국 학습시 해당 행렬들을 양자화하여 출력을 낸다. Qa는 vanilla min-max quant, Qw는 LWC다\n- Attention operation(EQUIVALENT) s_a는 어텐션 전용 스케일링 벡터다. 앞서 언급했다 싶이 s scaling으로 outlier를 최소화 시키기 위해서 학습가능한 s를 설정한다. s_a ∈ R^1xC_out FFN Layer에서 FC를 LET로 바꾸지 않은 이유는 두 연산의 행렬은 sparse하기에 성능에 악형향을 끼친다는 이유로 바꾸지 않았다. Main result 가중치 양자화 가중치, 활성화 양자화 다른 양자화 방법론들 보다 instruction-tuned 모델의 LOW-BIT에서의 성능이 좋다. Local device에서도 빠르게 돌아간다. Personal thought 왜 굳이 shift를 적용시키나? scaling만으로도 충분한 성능이 나오지 않나? 특히 코드상에서는 shift를 Layer norm에서도 적용이 가능하게 만든걸 봐서는 shift를 수식상에서는 사라지게 만든것같은데 굳이 이게 필요하나?\nB에 δW를 더해야 하는가? 새로운 파라미터를 넣거나 B를 뺐어도 되지 않았나?\n","permalink":"https://new-sunset-shimmer.github.io/OMNIQUANT/","summary":"PTQ의 효율성을 가지면서 QAT의 성능을 가지는 양자화 방법론","title":"OMNIQUANT"},{"content":" Full Title : OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models\nLink : 2306.02272 (arxiv.org)\nRelated Link : 2208.11580 (arxiv.org), 2305.14314 (arxiv.org), https://github.com/xvyaward/owq/blob/main/owq/recon.py#L13\nReleased Date : Sun, 4 Jun 2023\nReview Date : Sun, 24 Nov 2024\nMixed-precision quantazation based on Hessian matrix\nMotivation Quantazation method is hard to down very low bit one year ago. This method find outlier(in this paper refer as weak column) by Hessian matrix and keep as fp16. So model can down to very low bit. Research questions Is model can be quantize as very low bit? Methodology In this paper Authors find a outlier by Hessian matrix. By equations 4 and 5, finding the loss of quantized model is highly related to Hessian matrix.(delta is mean W-W^hat). By above intuition authors find outlier column(Weak column). They determine columns by sensitivity(equation 6). multiply by lambda(j-column). Lambda is meaning diagonal element of the Hessian matrix. Only keep unquantized top k column that most sensitive.\nIt is negligible small percentage(in this paper 0.3%) on speed.\nWe can observe weak column is same as outlier. Also some column is not huge magnitude. Weak Column Tuning is Training in quantized. Only train excluded column that named weak column. Qunatized column’s are frozen. Main result Based quantazition method is OPTQ\nPerplexity on WikiText2(Model-OPT) Perplexity on WikiText2(Model-LLaMA) Accuracy on Zero-shot accuracy(on MMLU, Model-OPT) LLaMa Accuracy on Zero-shot accuracy. WCT vs QLoRA and base, LoRA Equation compration. If perserve many column it is affected to models Group-wise qunatization Main proof ","permalink":"https://new-sunset-shimmer.github.io/OWQ/","summary":"Mixed-precision quantization based on Hessian matrix","title":"OWQ"},{"content":" Full Title : PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE MODELS\nLink : 2310.00034 (arxiv.org)\nRelated Link :\nReleased Date : Fri, 29 Sep 2023\nReview Date : Thu, 5 Dec 2024\nPartial binarize the model\nMotivation There is many research on quantization. Recent research prove that LLM can preserve linguistics ability even if weight are binary. in this paper challenging to extremely low bit. Authors quantize non-salient weight into one-bit, and keep fp16 salient weights. Also QAT that partial-binarized model. Research questions Can we binarize model ? Methodology PRELIMINARY: NETWORK BINARIZATION\nWe can make weight to binarize using sign(-) function. in forward A_F,o=W_b * A_F,i, - In the backward propagation, the sing(-) function is non-differentiable. For solve that problem researchers use Straight-through estimator(STE)(equation 2). It is numerically approximate the derivative of while BNN - In this paper implement binarization llm naively. from figure 2 authors implement recent binarization method to opt-1.3B model they are worse than just randomly chosen answer PARTIALLY BINARIZED WEIGHT MATRIX\noutlier(in this paper refer as salient weight) extend quantization range unnecessarily. Recent researches(late 2023, early 2024) exclude them from quantization. Concurrently make outlier more smoother, flatter for not extend the quantization range. in this paper authors exclude them from binarization.\nSALIENT WEIGHT: CRITERIA, GRANULARITY, AND COST\nCriteria(a principle or standard by which something may be judged or decided.) Authors use magnitude based salient weight chooser. It is not different from expensive Hessian based method\nGranularity(the scale or level of detail present in a set of data or other phenomenon.) : There is two method select salient weight as range. one is Element-wise, Column-wise. Authors elect Element-wise method. From Figure 3 salient weights are appear in random index and uniform. In column-wise can’t filter salient weight.\nSalient weight Storing cost can calculated by equation 3. if we binarize 90 weight and quantize 10 as 8bit, at most, 2.7-bit POST-TRAINING QUANTIZATION FOR PB-LLMS\nMost important thing is recover performance after PTQ.\nAuthors use GPTQ for finding what is important(salient) weight.\nIn GPTQ they are calculate from Hessian matrix layer by layer. from equation 4 they are make calculate gradient (equation 5). Determine which value is important in layer(column). Authors binarize non-salient value in layer and quantize salient value found by equeation w=w^2/[H-1]^2. and save as W = Wsal + Wunsal. Authors named this as PB-QPTQ\nQUANTIZATION-AWARE TRAINING FOR PB-LLMS\nSALIENT WEIGHTS FROZEN\nWhile training in quantization authors freeze salient weights because that weights are very important to model keep them as original value make train more efficient(Figure 5).\n- OPTIMAL SCALING FACTOR FOR BINARY WEIGHTS. Instead of training in general way they train scaling factor for binarized weights wF = αw¯ B.\nThe optimal values of scaling factor α for the W in binarization can calculated by minimizing the L2 error. from equation XNOT-Net Wb is binarized values In that case we can calculate alpha as eaquation 9 train by OPTIMAL SCALING FACTOR FOR BINARY WEIGHTS we can preserve linguistic ability of model preserve salient-weight and freeze it in train make very effective Main result Accuracy on PB-GPTQ and PB-LLM perplexity on PB-GPTQ got different salient weight preserve rate heuristic base of why authors chose hessian based quantize method. Personal thought why salient are uniform ? ","permalink":"https://new-sunset-shimmer.github.io/PB-LLM/","summary":"Partial binarize the model","title":"PB-LLM"},{"content":"2309.14717 (arxiv.org)\nQA-LORA: QUANTIZATION-AWARE LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\nReleased Date : 2023 10 09\nReview Date : 2024 10 19\n양자화된 Lora\nMotivation LLM의 높은 성능에도 불구하고 많은 계산비용과 높은 사이즈는 큰 걸림돌이 된다. 추가로 미세조정도 그 학습 데이터 크기에 비해 압도적으로 큰 모델은 종종 그 비용을 높게 만드는 원인중 하나가 된다. 그렇기에 양자화, Parameter-efficient fine-tunening(peft)가 많이 사용된다. 이번 논문에서는 모델 양자화와 양자화된 Lora를 적용시켜 두 문제를 해결할려고 한다. Methodology 해당 모델에서는 GPTQ를 사용하여 PTQ를 시행한다. QLoRA를 코드를 기준으로 작성하였다. 이번 논문에서는 GPTQ가 FP16이기에 양자화된 가중치에 더하기 위해서 PTQ가 변환하는 과정자체가 성능, 속도 측면에서 손실을 만드다고 서술했다. 그렇기에 LORA자체도 양자화하여 속도 측면에서 높은 우월성을 가지게 했다. 하지만 NF4로 양자화시 해당 타입을 위한 최적화 된 수식들이 없고 동시에 낮은 비트에서 성능이 굉장히 낮다는 단점이 있다. 저자들은 속도와 성능 그리고 사이즈 압축 측면을 해결하기 위해서 그룹 양자화라는 방법을 제안했다. Dout x K 의 Dout을 여러 그룹으로 쪼개 양자화를 하는 방식이다. Dout x K Main result 모델은 LLama와 LLaMA2를 사용. MMLU 벤치 마크에서의 zero-shot이랑 few-shot에서의 성능을 확인했으며 동시에 여러 과학 벤치마크도 사용했다. common sense에서의 reasoning ability도 확인.\nQLORA랑 같은 세팅을 했다. 미세조정은 FLAN v2, Alpaca를 사용했다.\n","permalink":"https://new-sunset-shimmer.github.io/QA-LORA/","summary":"양자화된 Lora","title":"QA-LORA"},{"content":" Full Title : QLLM: ACCURATE AND EFFICIENT LOW-BITWIDTH QUANTIZATION FOR LARGE LANGUAGE MODELS\nLink : 2310.08041 (arxiv.org)\nRelated Link :\nReleased Date : Thu, 12 Oct 2023\nReview Date : Thu, 5 Dec 2024\nSplitting outlier to sub channels\nMotivation Outlier(huge magnitude value) make quantization harder. In this paper authors distribute huge magnitude to all over layers. Author can make outliers more smoother than SmoothQuant Research questions Can we make outlier smoother ? Methodology ADAPTIVE CHANNEL REASSEMBLY\nFor distribute input’s outlier to multiple channel consists of three components. 1) decompose 2) assembling for balancing the efficiency 3) Adaptively find reassembly ratio for each layer\nCHANNEL DISASSEMBLY Authors splitting outlier to T - 1 sub-channels and adding them. this equation equivalent with original but more quantization friendly .\nT is determined by threshold value θ. [T = ⌈max(|xM|)/θ⌉.]\nCHANNEL ASSEMBLY But from Channel disassembly Total channel count increased to M + T - 1. for maintaining efficiency and speed prune channels. Prune unimportant channels directly but this method make loss unstable(also gradient). Authors use combining T - 1 similar tokens inorder to preserve total channel counts\nWe can aggregate channels by above equation. Average of their input features, Wik + Wjk is merged weights\n∥·∥2 represents the ℓ2 norm. we minimizing the information loss of two weight.\nafter calculate all channels determine which channels to aggregate efficiently use Bipartite soft matching but not assemble outlier channels\nADAPTIVE REASSEMBLY for find optimal θ authors solve equation 6 by grid search\nEFFICIENT GRADIENT-BASED ERROR CORRECTION\nfor make assembled weight more powerful authors train low rank AB in small amount calibrated dataset.\nlearn the introduced low-rank parameters by minimizing the reconstruction error between the original and the quantized outputs of the Attention-FFN block\nand store only quantized weight.\nMain result use per-channel weight quantization and per-token activation quantization. we quantize all weights and intermediate activations, with the exception of the Softmax output probability\nQLLM begins by applying channel reassembly prior to all linear projection layers, excluding the attention output projection layer, followed by performing error correction on the resulting model. The rank r of the introduced low-rank parameters is set to 4, and these parameters are trained for 10 epochs with a mini-batch size of 1. We carry out the reconstruction using 4 Attention-FFN blocks. use AdamW with lr 5 x 10^-4\nPerformance of QLLM Performance on channel each method. Channel expansion ration is parameter that how much expand channel by delta. Throughtput TQW which tunes all quantized weights, EEC focuses on learning a small set of low-rank weights Personal thought why it is working ? just splitting outliers and quantize them is sound unstable and make no sound but it is working ","permalink":"https://new-sunset-shimmer.github.io/QLLM/","summary":"Splitting outlier to sub channels","title":"QLLM"},{"content":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference (arxiv.org)\nQuest: Query-Aware Sparsity for Efficient Long-Context LLM Inference\nReleased Date : 2024 08 26\nReview Date : 2024 10 04\nKV Cache에서 필요없는 token 버리기\nMotivation Self attention에서 KV Cache는 Sequence가 길경우 많은 메모리를 사용한다. 해당 문제를 해결하기 위해서 Token을 버리는 방법을 저자들은 제안했다. Static하게 Token을 버릴 경우 미래에 필요한 Token도 같이 버려질 가능성이 높기에 Query 기반으로 버리는 방법을 제안한다. Methodology KV에서 K를 기준으로 가장높은 키값과 가장 낮은 키값을 Query와 Element-wise Product 연산을 통해서 값을 구한다. Element-wised Matrix에서 가장 높은 값들만 구한뒤 Sum한다. Top-k의 page만 로드하면 된다. Main result ","permalink":"https://new-sunset-shimmer.github.io/Quest/","summary":"KV Cache에서 필요없는 token 버리기","title":"Quest"},{"content":" Full Title : QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\nLink : QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models (arxiv.org)\nRelated Link : IST-DASLab/QUIK: Repository for the QUIK project, enabling the use of 4bit kernels for generative inference - EMNLP 2024 (github.com)\nReleased Date : Wed, 4 Dec 2024\nReview Date : Fri, 13 Oct 2023\nExclude Outlier and improve speed by custom kernels\nMotivation For better quantized LLM we should quantize weight and activation for compute-bound. In this paper authors quantize Activation and weight into 4 bit exclude outliers.\nonly small amount of inputs are memory bound. Rest of input size is hugely related to Compute bound\nQuantizing activation is improve llm speed geatly Research questions Can we make LLM’s compute-bound more loose ? Methodology Authors quantize weights and activation. They use GPTQ in this paper that also exclude outliers when quantize the weights. But this is very slow in computation and quantize.\nIn this paper they reorder outliers to last of(in this paper named right of column) column. this help GPTQ aggregate to right columns that make quantization error and remove from quantize\n- Weight Clipping Weight clipping when quantize there is two way to apply, First, training omptimal clipping value, Second find it by heurtics. Authors use linear search over the clipping thresholds. - Sensitivity-Based Partial Quantization Finding outliers is key in this paper. Authors use norm to find outliers. they select largest norm values. Also Down-proj layers are very sensitive and important but this layer is get poor performance when quantize in 4 bit. Authors quantize Down-proj as int 8 instead of 4 bit Efficient Inference Implementation\nQuantization\nIn this paper use two method. Quantize weights symmetrically(only scale) in offline, quantize activations asymmetrically(scale and zero) in online.\nMatrix Multiplication\nThe actual MatMul is performed by the CUTLASS (NVIDIA, 2023) library, which is able to effectively utilize the hardware’s INT8/INT4 tensor-cores to perform fast low-precision calculations, while accumulating results in a wider INT32 format.\nDequantization\nAuthors refer we need to dequantize a weight in order to calculate Scale, zero for activation. multipling scaleAct and scaleWeight to inputInt and considering Zeroact we need to add constant value * wReduced.(equation 1 is for account for activation zero point)\nFlow of Quik Main result use GPTQ, finding Clipping thresholds for weight quantization are found via a linear search over the squared error\noperation time per operation\nPPL of Quik Zero-shot accuracy PPl on WIKItext2 when model is 8 bit ppl(Thresholds) Peak memory usage(GB) speed of per layer size speed of models layers speed 4-bit Down-Proj diffrent outliers numbers(This mean make more fp16 layer that excluded) Make MLP BLock 2:4 sparse. this isnt affect to overall performance than attn block Personal thought i think there is sensitive layer in model. ","permalink":"https://new-sunset-shimmer.github.io/QUIK/","summary":"Exclude Outlier and improve speed by custom kernels","title":"QUIK"},{"content":" Full Title : QuIP: 2-Bit Quantization of Large Language Models With Guarantees\nLink : 0df38cd13520747e1e64e5b123a78ef8-Paper-Conference.pdf (neurips.cc)\nRelated Link : Up or Down? Adaptive Rounding for Post-Training Quantization (arxiv.org) QuIP: 2-Bit Quantization for LLMs - YouTube\nReleased Date : 2023 06 25\nReview Date : 2024 11 04\nHessian matrix을 이용한 AdaptRound 최적화\nMotivation 이전연구인 Up or Down? Adaptive Rounding for Post-Training Quantization (arxiv.org)에서 제시한 Adaptive Round을 최적하는 방법론을 제시한다. Research questions 2bit로 최적화가 가능한가? Methodology 이전논문인 Up or Down? Adaptive Rounding for Post-Training Quantization (arxiv.org)에서는 Layer단위의 양자화 손실을 구하는 식 2를 확장시켰다. 확장시킬 때 TAYLOR series를 이용하였다. (a)는 Taylor series를 활용하여 확장했다는 듯이다. Tayelor series는 어떠한 함수의 근사로서 사용될수있다. 아래의 그림을 보면 여기서는 e^x를 근사하고있다. Tayelor Series는 Local minimum을 보장한다는 성질있다.[확실하지는 않음]. 그렇기에 근사할려는 함수를 보다 빠르게 local minimum을 찾을수있다. 위의 식 4를 이전 연구The unconstrained binary quadratic programming problem: a survey | Journal of Combinatorial Optimization (springer.com)을 따라 아래와 같은 식을 유도할수있다. H는 Hessian matrix이다. 또한 H는 Memory complexity가 높기에 해당 논문(upper, down)에서는 다른 식을 유도하여 사용한다. Hessian matrix는 어떠한 매트릭스의 2차 미분이다. 이는 1차 미분인 gredient처럼 어느 방향으로 얼마나 갚파른지를 확인할수있다. 하지만 gredient가 직선이라면 Hessian matrix은 굴곡졌다. 이번 논문인 QuIP에서는 위의 식 13을 그대로 쓴다. 이는 H의 Memory complexity를 무시하고도 쓸 가치가 있다는 이야기다. 저자들은 H가 가중치와 incohirence하여 보다 빠르게 local minimum을 안정적이고 확실하게 찾을수있다고 서술했다. 이 모든 과정을 Guarante 즉 수학적으로 증명하여 확실하게 모든 LLM에 적용이 된다는 걸 서술했다. 모든 식과 증명을 이번 요약에서 설명하면 좋겠지만 배움의 한계로 힘들어서 생략하겠다. 결국 이번 논문에서의 가장 큰 아이디어는 직교행렬들의 Kronecker 곱셈을 이용하여 H와 가중치가 incohirence하다는 걸 보장한다.\n앞서 언급했듯 H는 일종의 gredient로서 작용이 가능하다. 이러한 매트릭스와 incogirence하다는 건 반대 해당 매트릭스가 H와 반대방향으로 달려간다는 뜻이다. 또한 H는 µ-incoherent positive semi-definite symmetric matrix다. Positive이라는 건 0 이상의 값들만 가지고, Symmetric은 H=H^-1가 동일하는 뜻이다. Symmetric으로 W와 H를 곱해줄때 W가 rotation이 안되도록 보장하며 Positive semi-definite는 해당 gredient의 방향과 형태를 보다 잘 나타낸다[chatgpt 피셜].\nKronecker 곱셈\n이래의 알고리즘을 확이하면 결국 1의 5번 2의 3번이 가중치와 H을 직교행렬로 곱해주는 연산이 중요하다. Adaptive rounding Main result 파라미터별 2bit로 양자화시 perplexity와 성능표. 모델의 파라미터가 높으면 높을수록 손실이 적다 모델의 양자화별 성능표 알고리즘의 각 기능별 성능(ppl) 비슷한 방법론인 OPTQ와의 Throughput 비교 Personal thought 수학적인 과정이 어렵다.\n이러한 수학적인 정보는 학부때 전부다 배우나? ㄹㅇ 궁금\n","permalink":"https://new-sunset-shimmer.github.io/QuIP/","summary":"Hessian matrix을 이용한 AdaptRound 최적화","title":"QuIP"},{"content":" 크롤링\nCRAWLER\nDB\nLLM Chat\nRetriever model\nLangchain\nInstruction tuning\nOUTPUT\n[BONUS]\n시현\nDisclaimer!!!!!!: all of my code based on https://github.com/Wilson-ZheLin/GPT-4-Web-Browsing\n크롤링 CRAWLER Spider을 통해서 url들을 돌아 다니면서 HTML파일들을 가져오고 SCRAPER을 통해서 HTML 파일에서 필요한 정보들을 가져와 DATABASE에 저장한다. requests와 BeautifulSoup로 간단하게 Scraper와 Crawler을 구현. DB는 관리가 쉬우며 동시에 여러 기기로 쉽게 옮길수있는 Langchain에 제공되고있는 Chroma를 썼다.\nDB Chroma:\nChroma는 오픈 소스 임베딩 데이터베이스입니다. Chroma를 사용하면 지식, 사실, 기술을 LLM에 플러그할 수 있게 만들어 LLM 앱을 쉽게 구축할 수 있습니다. 출처:🏡 Home | Chroma (trychroma.com)\n문서들을 Vector로 Embedding 하여 저장하고 Retrieve할 때도 Query를 Embedding하여 vector기반으로 관련이 높은 문서들을 찾는다. Embedding을 즉 Vector을 만들때 인공지능 모델을 쓰는 경우가 있으며 https://github.com/ssisOneTeam/Korean-Embedding-Model-Performance-Benchmark-for-Retriever 의 성능 지표를 따라 가장 성능이 높은 jhgan/ko-sroberta-multitask을 쓰고 있다.\nLLM Chat Retriever Model 앞서 설명한 langchain에서 Chroma DATABASE를 그대로 Retriever Model로서 쓸수있다. Query를 Vector로 변환시키고 그 Vector와 가장 유사한 TOP k개의 문서들을 찾는다.\nLLM LLM은 대규모 언어 모델이다. Query와 TOP K의 문서들을 기반으로 정보를 제공한다. 정보를 제공하기 위해서 IN-CONTEXT LEARNING으로 TEMPLATE을 작성하여 PROMPT TUNING으로 성능을 올리려고한다.\nPROMPT TUNING PROMPT TUNING을 위한 TEMPLATE은 아래와 같은 방식으로 작성했다.\nGEMINI, GPT4o template: | Web search result: {context_str} Personal Information: {information} Instructions: You are a/an {profile}. Using the provided web search results and Personal Information, write a comprehensive and detailed but not long text reply to the given query. Make sure to cite results using [number] notation after the reference. At the end of the answer, list the corresponding references with indexes, each reference contains the urls and quoted sentences from the web search results by the order you marked in the answer above and these sentences should be exactly the same as in the web search results. We also provide a Personal Information, if user ask any self information then provide from Personal Information and dont reference any link. Here is an example of a reference: [1] URL: https://www.pocketgamer.biz/news/81670/tencent-and-netease-dominated-among-chinas-top-developers-in-q1/ self information example: 학번, 이름, 학과, 성적 Answer in language: {language} Query: {query} Output Format: {format} Please organize your output according to the Output Format. If the Output Format is empty, you can ignore it. key_word_template: | Instructions: You are a/an {profile}. Make keyword of {query} constucted by under 5 tokens. Keyword mustn\u0026#39;t be answer of {query}. Keyword must be main context of {query} and must understandable by human. There is no need to add another text with keyword when answering. Answering only keyword. Here is an examples of how to make keyword: Question -\u0026gt; keyword [1] 전북대학교 컴퓨터공학부 졸업 조건을 알려줘 -\u0026gt; 컴퓨터공학부 졸업 조건 [2] 지금 휴학 가능한가요? 일반휴학입니다 -\u0026gt; 휴학 [3] 자퇴신청 어제 까진데 또 언제할수있음? -\u0026gt; 자퇴신청 일정 [4] 전북대학교 영어영문학과 졸업 조건을 알려줘 -\u0026gt; 영어영문학과 졸업 조건 [5] 내 학번을 알려줘 -\u0026gt; 학번 질문 Answer in language: {language} Query: {query} Output Format: {format} search_template: | Instructions: please classify(labeling) {query} by comprehensive enough to answer. 1 is label of comprehensive enough and 2 is NOT comprehensive enough. Here is an examples of sentences labeled for help to classify. but not too srictly classify to 2: query : label 저학년이 고학년 벨트 미리 딸 수 있나요 제가 2학년인데요 : 1 교수님께서 lms에서 개인별 성적란 누르면 확인된다하셨는데 성적누르면 해당 강의에 대한 성적은 비공개되어 확인할 수 없습니다라고 떠요 어디서 볼수있나요? 메일 드려야하나요? : 1 과목코드는 어디서 봐? : 2 대학생활계획서는 어디서 써? : 2 전진설 4번 하는거 2,3 학년말고 4학년에 하는것도 포함해서 4번하는건가요? 2,3학년에 3번하고 4학년에 1번해도 되는건가요? : 2 중도 앞에서 오늘 행사 함? : 1 오늘 휴학 가능할까? : 1 링크를 요약해줘 : 1 Answer in language: {language} Query: {query} Output Format: [ class of sentence : Reason: Solution: ] Llama3, other local LLM template: | \u0026lt;|begin_of_text|\u0026gt; \u0026lt;|start_header_id|\u0026gt; system \u0026lt;|end_header_id|\u0026gt; Web search result: {context_str} Personal Information: {information} Instructions: You are a/an {profile}. Using the provided web search results and Personal Information, write a comprehensive and detailed reply to the given query. Make sure to cite results using [number] notation after the reference. At the end of the answer, list the corresponding references with indexes, each reference contains the urls and quoted sentences from the web search results by the order you marked in the answer above and these sentences should be exactly the same as in the web search results. We also provide a Personal Information, if user ask any self information then provide from Personal Information and dont reference any link. Here is an example of a reference: [1] URL: https://www.pocketgamer.biz/news/81670/tencent-and-netease-dominated-among-chinas-top-developers-in-q1/ Quoted sentence: Tencent accounted for roughly 50% of domestic market revenue for the quarter, compared to 40% in Q1 2022. self information example: 학번, 이름, 학과, 성적 Answer in language: {language} Output Format: {format} Please organize your output according to the Output Format. If the Output Format is empty, you can ignore it. \u0026lt;|eot_id|\u0026gt; \u0026lt;|start_header_id|\u0026gt; user \u0026lt;|end_header_id|\u0026gt; Query: {query} Answer in language: {language} \u0026lt;|eot_id|\u0026gt; \u0026lt;|start_header_id|\u0026gt; assistant \u0026lt;|end_header_id|\u0026gt; key_word_template: | \u0026lt;|begin_of_text|\u0026gt; \u0026lt;|start_header_id|\u0026gt; system \u0026lt;|end_header_id|\u0026gt; Instructions: You are a/an {profile}. Make keyword of {query} constucted by under 5 tokens. Keyword mustn\u0026#39;t be answer of {query}. Key word must be main context of {query} and must understandable by human. There is no need to add another text with keyword when answering. Answering only keyword. Here is an examples of how to make keyword: Question -\u0026gt; keyword [1] 전북대학교 컴퓨터공학부 졸업 조건을 알려줘 -\u0026gt; 컴퓨터공학부 졸업 조건 [2] 지금 휴학 가능한가요? 일반휴학입니다 -\u0026gt; 휴학 [3] 자퇴신청 어제 까진데 또 언제할수있음? -\u0026gt; 자퇴신청 일정 [4] 전북대학교 영어영문학과 졸업 조건을 알려줘 -\u0026gt; 영어영문학과 졸업 조건 [5] 내 학번을 알려줘 -\u0026gt; 학번 질문 Answer in language: {language} Output Format: {format} \u0026lt;|eot_id|\u0026gt; \u0026lt;|start_header_id|\u0026gt; user \u0026lt;|end_header_id|\u0026gt; Query: {query} \u0026lt;|eot_id|\u0026gt; \u0026lt;|start_header_id|\u0026gt; assistant \u0026lt;|end_header_id|\u0026gt; OUTPUT EXAMPLE GPT4o’s EXAMPLE OUTPUT Q:계절학기 수강신청 완료 확인 지금 안되는 거 맞아?? 취업교과목 하나 신청 했는데 안뜨는 것 같아서 A:{\u0026#39;answer\u0026#39;: \u0026#39;계절학기 수강신청 완료 확인이 안 되는 상황에 대해 걱정이 많으시겠네요. 전북대학교의 계절학기 수강신청 관련 정보를 참고하여 답변드리겠습니다.\\n\\n전북대학교의 계절학기 수강신청은 보통 5월과 11월 초순에 진행됩니다. 수강신청 후 수강료 납부, 폐강과목 공고 및 수강정정 등의 절차가 이어집니다. 수강신청 인원이 30명 이상인 경우에만 강의가 개설되며, 전공과목의 경우 20명 이상으로 변경되었습니다[1][2].\\n\\n수강신청이 완료되었는지 확인하려면, 전북대학교 오아시스 시스템에 로그인하여 수강신청 내역을 확인해보시기 바랍니다. 만약 수강신청 내역이 보이지 않는다면, 수강신청이 제대로 완료되지 않았을 가능성이 있습니다. 이 경우, 학교의 학사관리팀이나 해당 학부에 문의하여 확인하는 것이 좋습니다.\\n\\n취업교과목을 신청했는데 수강신청 내역에 나타나지 않는다면, 다음과 같은 절차를 따라 확인해보세요:\\n1. 오아시스 시스템에 로그인합니다.\\n2. 수강관리 메뉴에서 수강신청 내역을 확인합니다.\\n3. 수강신청이 제대로 완료되지 않았다면, 수강신청 기간 내에 다시 시도하거나 학사관리팀에 문의합니다.\\n\\n추가로, 계절학기 수강신청 관련 공지사항을 확인하여 변경된 사항이나 추가 안내를 참고하시기 바랍니다[3].\\n\\n참고로, 전북대학교의 계절학기 수강신청 관련 주요 내용은 다음과 같습니다:\\n- 수강자격: 본교 및 협력대학 재학생, 본교 휴학생\\n- 신청학점: 6학점 이내\\n- 개설과목: 교양과목 및 전공과목\\n- 설강기준: 수강신청 인원 30명 이상 (전공과목은 20명 이상)\\n- 수업시간: 3학점의 일반강의 과목 기준으로 1주당 15시간 (총 3주 45시간)[1][2].\\n\\n이 정보를 바탕으로 수강신청 내역을 다시 확인해보시고, 문제가 지속될 경우 학교에 문의하시기 바랍니다.\\n\\n### References\\n[1] URL: https://www.jbnu.ac.kr/kor/?menuID=139\u0026amp;category=%EA%B8%B0%ED%83%80\u0026amp;pno=2388\\n Quoted sentence: \u0026#34;수강자격 : 본교 및 협력대학 재학생, 본교 휴학생(단, 휴학생의 경우 당해학기 계절수업을 이수하여 졸업학점을 충족하더라도 졸업은 불가)\u0026#34;\\n[2] URL: https://www.jbnu.ac.kr/kor/?menuID=140\u0026amp;category=%EA%B5%90%EC%9B%90%EC%B1%84%EC%9A%A9\u0026amp;pno=204\\n Quoted sentence: \u0026#34;설강기준 : 수강신청 인원 30명 이상인 경우 설강\u0026#34;\\n[3] URL: https://top.jbnu.ac.kr/bbs/bie/1180/301133/artclView.do?layout=unknown\\n Quoted sentence: \u0026#34;계절수업 개설 기준 인원이 변경되었으니 확인하시어 신청 시 참고하여 주시면 감사하겠습니다. 전공과목 : 30명 → 20명 교양과목 : 30명 유지\u0026#34;\\n\u0026#39;, \u0026#39;keyword\u0026#39;: \u0026#39;계절학기 수강신청 확인\u0026#39;} Q:전북대 홈페이지에 5월 1일에 중간고사 완료라고 뜨는데.. 정확히 며칠부터 중간고사 기간인가요? A:{\u0026#39;answer\u0026#39;: \u0026#39;전북대학교의 학사일정에 따르면, 중간고사 기간은 명확히 명시되어 있지 않습니다. 다만, 중간고사 완료일은 5월 1일로 표시되어 있습니다[5]. 일반적으로 중간고사는 학기 중반에 시행되며, 보통 8주차에 시작됩니다. 따라서, 중간고사 기간은 4월 중순부터 5월 초까지로 예상할 수 있습니다.\\n\\n### References\\n[5] URL: https://likehome.jbnu.ac.kr/home/main/inner.php?sMenu=D5000\u0026amp;mode=view\u0026amp;no=5349\\n Quoted sentence: \u0026#34;전북대학교 학사일정에 시험기간이라는 것은 명시 되어있지 않습니다. 그렇기 때문에 야간출입허용기간은 학사일정에 명시되어있는 중간시험 종료를 기점으로 합니다.\u0026#34;\\n\u0026#39;, \u0026#39;keyword\u0026#39;: \u0026#39;중간고사 기간\u0026#39;} GEMINI’s EXAMPLE OUTPUT Q:전북대학교 컴퓨터공학부 졸업 조건을 알려줘 A:전북대학교 컴퓨터공학부 졸업 조건은 다음과 같습니다. 1. 총 130학점 이상 이수 2. 전공 필수 과목 42학점 이상 이수 3. 전공 선택 과목 24학점 이상 이수 4. 교양 과목 30학점 이상 이수 5. 실무 훈련 6학점 이상 이수 6. 졸업 논문 6학점 이상 이수 [1] 2015년 : 전북대 IT정보공학과 공학 학사 2017년 : 전북대 컴퓨터공학과 공학 석사 2017년 ~ 현 : 전북대 컴퓨터공학과 박사과정 관심 분야: privacy-preserving, cloud computing, DNN 2019년 ~ 현 : IT정보공학과 학부 과정 관심 분야: cloud computing, AI, DNN Q:나의 학과는 뭐임? A:컴퓨터인공지능학과 [1] [1] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=9 Quoted sentence: The Journal of Korean Institute of Communications and Information Sciences Llama3 8B 4bit-Quant instruct Q:전북대학교 컴퓨터공학부 졸업 조건을 알려줘 A:According to the provided web search results, the graduation requirements for Jeonbuk National University\u0026#39;s Computer Science Department are as follows: Firstly, it is mentioned that the department has a comprehensive design project development requirement for graduates. This suggests that students must complete a significant project that showcases their skills and knowledge in computer science. Secondly, the department\u0026#39;s webpage [2] states that the graduation requirements include satisfying the credits required for the degree, which implies that students must complete a certain number of credits or courses to graduate. Lastly, the department\u0026#39;s webpage [3] and [4] mention that visitors are welcome to explore the department\u0026#39;s website, but it does not provide specific information on the graduation requirements. However, it is likely that the department has its own set of guidelines and regulations for graduation. As a conscientious researcher, I would recommend checking with the university or the department directly for the most up-to-date and accurate information on the graduation requirements. References: [2] URL: https://it.jbnu.ac.kr/it/14016/subview.do Quoted sentence: 전북대학교 학수구분별 졸업학점 [3] URL: https://cse.jbnu.ac.kr/cse/3577/subview.do Quoted sentence: 컴퓨터공학부를 방문해주신 고등학생, 학부모, 입시담당 선생님께 감사드립니다. [4] URL: https://csai.jbnu.ac.kr/cse/3577/subview.do Quoted sentence: 컴퓨터공학부를 방문해주신 고등학생, 학부모, 입시담당 선생님께 감사드립니다. Q:나의 학과는 뭐임? A:Your department is 컴퓨터인공지능학과 (Computer and Information Sciences). According to your personal information, you are a student of this department. References: [None] maywell/Llama-3-Synatra-11B-v1 Q:전북대학교 컴퓨터공학부 졸업 조건을 알려줘 A:전북대학교 컴퓨터공학부 졸업 조건은 다음과 같아요. 먼저, 전북대 컴퓨터공학부에서는 졸업 조건을 만족하기 위해 학생들은 종합설계 작품을 개발해야 해요. 이 작품은 학생들이 학습한 지식과 기술을 실제 문제 해결에 적용하는 것을 목표로 하며, 이를 통해 학생들은 실제 세계의 문제를 해결할 수 있는 능력을 키울 수 있어요. 또한, 졸업 조건을 만족하기 위해서는 학생들은 졸업자격 인증제 이수기준을 만족해야 해요. 이 기준은 학생들이 전북대에서 취득한 학점을 기반으로 하며, 이를 통해 학생들이 전북대에서 충분한 학업을 이수했는지를 판단해요. 전북대 컴퓨터공학부 졸업 조건은 이러한 두 가지 요소를 포함하며, 학생들이 실제 세계의 문제를 해결할 수 있는 능력을 키울 수 있도록 도와줘요. 더 궁금한 점이 있으면 언제든지 물어봐 주세요! References: [1] [https://sites.google.com/view/jbnu-dblab/people](https://sites.google.com/view/jbnu-dblab/people) [2] [https://it.jbnu.ac.kr/it/14016/subview.do](https://it.jbnu.ac.kr/it/14016/subview.do) [3] [https://cse.jbnu.ac.kr/cse/3577/subview.do](https://cse.jbnu.ac.kr/cse/3577/subview.do) [4] [https://csai.jbnu.ac.kr/cse/3577/subview.do](https://csai.jbnu.ac.kr/cse/3577/subview.do) [5] [https://csai.jbnu.ac.kr/csai/29037/subview.do](https://csai.jbnu.ac.kr/csai/29037/subview.do) Q:나의 학과는 뭐임? A:컴퓨터인공지능학과에요. [1] [2] [3] [4] [5] [6] Mistral instruct 7B 4bit-Quant instruct Q:전북대학교 컴퓨터공학부 졸업 조건을 알려줘 A:전북대학교 컴퓨터공학부에서의 졸업 조건에 대해 알아드리겠습니다. according to the provided web search results and Personal Information. First, let me clarify that there are two different programs within the Computer Science Department at Jeonbuk National University: the Computer Science program ([Webpage[3], [Webpage[4]]) and the Computer Engineering and Artificial Intelligence program ([Webpage[5]]). Based on the Personal Information provided, you are enrolled in the Computer Engineering and Artificial Intelligence program. Now, let\u0026#39;s discuss the general graduation requirements for Jeonbuk National University students as mentioned in [Webpage[2]]. To graduate, a student must earn the required number of credits, obtain the necessary certifications, complete a comprehensive design project, and meet other specific departmental requirements. However, the details of these requirements may vary depending on the specific program. For Computer Engineering and Artificial Intelligence students, the focus areas are privacy-preserving, cloud computing, and deep neural networks (DNN) as per [Webpage[1]]. It is essential to note that the Personal Information provided indicates that you are currently pursuing a Bachelor\u0026#39;s degree in IT Information Science within this department. The graduation requirements for undergraduate students may differ from those of graduate students. To summarize, the specific graduation requirements for Computer Engineering and Artificial Intelligence students at Jeonbuk National University can be found by consulting the departmental guidelines or speaking with an academic advisor. These requirements likely include earning a certain number of credits, completing specific coursework in privacy-preserving, cloud computing, DNN, and other related fields, and potentially completing a thesis or capstone project. References: [1] URL: https://sites.google.com/view/jbnu-dblab/people Quoted sentence: 관심 분야: privacy-preserving, cloud computing, DNN [2] URL: https://it.jbnu.ac.kr/it/14016/subview.do Quoted sentence: 졸업자격인증 인증구분별 졸업학점 (2024.2월 졸업자까지만 적용) [3] URL: https://csai.jbuku.ac.kr/cse/3577/subview.do Quoted sentence: 컴퓨터공학부를 방문해주신 고등학생, 학부모, 선배 여러분 감사합니다. [4] URL: https://csai.jbuku.ac.kr/cse/3577/subview.do Quoted sentence: 컴퓨터공학과에서는 컴퓨터 공학, 소프트웨어 공학, 인форма션 통신 공학 등 다양한 분야로부터 전문가를 양성하고 있습니다. [5] URL: https://csai.jbuku.ac.kr/cse/3577/subview.do Quoted sentence: 컴퓨터공학과에서는 인간-컴퓨터 인터페이스, 데이터베이스 시스템, 네트워크 기술, 운영 체제, 소프트웨어 공학, 컴퓨터 구조와 설계, 인공지능, 데이터 분석 등 다양한 분야에서 전문가를 양성합니다. Q:나의 학과는 뭐임? A:당신의 학과는 \u0026#34;컴퓨터인공지능학과\u0026#34;입니다. [1] 이 학과에서 최근에 발표된 논문들에 대해 소개하겠습니다. [1] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: In this issue, we present papers on various topics including a new energy-based latent-variable model for unsupervised feature learning [2], jamming detection algorithm using the kurtosis of the received signal [3], rate-matching precoder design based on rate-splitting multiple access for heterogeneous traffic demands [4], performance evaluation for UAV-Based UL-NOMA with channel coding [5], performance analysis of signal source direction finding algorithms for PS-LTE networks [6], hybrid beamforming in Sub-THz LoS MIMO systems [7], on uplink radio resource allocation for prioritized two-step random access [8], development of robust crack segmentation and thickness measurement model using deep learning [9], domain adaptive deep learning-based crack detection for building inspection [10]. References: [1] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: In this issue, we present papers on various topics including a new energy-based latent-variable model for unsupervised feature learning [2], jamming detection algorithm using the kurtosis of the received signal [3], rate-matching precoder design based on rate-splitting multiple access for heterogeneous traffic demands [4], performance evaluation for UAV-Based UL-NOMA with channel coding [5], performance analysis of signal source direction finding algorithms for PS-LTE networks [6], hybrid beamforming in Sub-THz LoS MIMO systems [7], on uplink radio resource allocation for prioritized two-step random access [8], development of robust crack segmentation and thickness measurement model using deep learning [9], domain adaptive deep learning-based crack detection for building inspection [10]. [2] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: A New Energy-Based Latent-Variable Model for Unsupervised Feature Learning [3] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: Jamming Detection Algorithm Using the Kurtosis of the Received Signal [4] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: Rate-Matching Precoder Design Based on Rate-Splitting Multiple Access for Heterogeneous Traffic Demands [5] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: Performance Evaluation for UAV-Based UL-NOMA with Channel Coding [6] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6 Quoted sentence: Performance Analysis of Signal Source Direction Finding Algorithms for PS-LTE Networks [7] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: Hybrid Beamforming in Sub-THz LoS MIMO Systems [8] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: On Uplink Radio Resource Allocation for Prioritized Two-Step Random Access [9] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: Development of Robust Crack Segmentation and Thickness Measurement Model Using Deep Learning [10] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5 Quoted sentence: Domain Adaptive Deep Learning-Based Crack Detection for Building Inspection phi3 4B 4bit-Quant instruct Q:전북대학교 컴퓨터공학부 졸업 조건을 알려줘 A:전북대학교의 컴퓨터공학부에서 졸업을 거래하는 조건은 다음과 같아요. [1] 생년도 2017년부터 전북대학교의 컴퓨터공학과 공학 강사로 이사하고, 그에 따라 전북대학교의 졸업 기계에서 공학 교육을 받아들여요. [2] 졸업자가 정보를 제공하면, 그는 전북대학교의 공학교육인지 검사하고, 공학 교육에서 확장되어질 수 있는 주제를 찾아내는 과정을 걸어요. [3] 전북대학교의 컴퓨터공학부를 방문하면, 졸업자가 전북대학교의 입학 본부로 이동하도록 하는 것을 기울시요. [4] 졸업자가 공학 교육에서 확장되어질 수 있는 주제를 찾아내는 과정은, 전북대학교의 공학 교육 기계에서 공학 교육을 받아들여요. [5] 전북대학교의 공학교육 기계는 공학 교육에 확장되어질 수 있는 주제를 찾아내고, 졸업자가 공학 교육에서 확장되어질 수 있는 주제를 찾아내는 과정을 걸어요. 이름: John Doe 학년도: Fall 2 Written by: David Sirota Published: April 1, 2009 The United States has a long history of using its military might to influence global politics and economics. From the Spanish-American War in 1898 to the invasion of Iraq in 2003, American interventionism has shaped the world we live in today. However, this approach is not without controversy. Critics argue that it often leads to unintended consequences and can undermine democratic values both at home and abroad. One example of U.S. military intervention was the invasion of Iraq in 2003. The Bush administration justified this action by claiming that Saddam Hussein\u0026#39;s regime possessed weapons of mass destruction (WMD) and posed a threat to regional stability. However, no WMD were ever found, leading many to question the true motives behind the invasion. The consequences of U.S. interventionism are far-reaching. In some cases, it has led to increased instability in regions where American forces have been deployed. For example, after the 2003 Iraq War, sectarian violence escalated and contributed to the rise of extremist groups like ISIS. Additionally, U.S. military interventions can strain relationships with allies and create resentment among local populations who may view American forces as occupiers rather than liberators. On the other hand, proponents argue that there are situations where U.S. intervention is necessary to protect human rights or promote democracy. For instance, during the Cold War, America\u0026#39;s involvement in conflicts like Korea and Vietnam was seen by some as a way to contain communism and prevent its spread. In conclusion, while American military intervention has played an important role in shaping global politics, it is essential for policymakers to carefully consider the potential consequences of such actions. Balancing national interests with respect for international norms and human rights should be at the forefront of any decision to deploy U. Written by: David Sirota Published: April 1, 2009 The United States has a long history of using its military might to influence global politics and economics. From the Spanish-American War in 1898 to the invasion of Iraq in 2003, American interventionism has shaped the world we live in today. However, this approach is not without controversy. Critics argue that it often leads to unintended consequences and can undermine democratic values both at home and abroad. One example of U.S. military intervention was the invasion of Iraq in 2003. The Bush administration justified this action by claiming that Saddam Hussein\u0026#39;s regime possessed weapons of mass destruction (WMD) and posed a threat to regional stability. However, no WMD were ever found, leading many to question the true motives behind the invasion. The consequences of U.S. interventionism are far-reaching. In some cases, it has led to increased instability in regions where American forces have been deployed. For example, after the 2003 Iraq War, sectarian violence escalated and contributed to the rise of extremist groups like ISIS. Additionally, U.S. military interventions can strain relationships with allies and create resentment among local populations who may view American forces as occupiers rather than liberators. On the other hand, proponents argue that there are situations where U.S. intervention is necessary to protect human rights or promote democracy. For instance, during the Cold War, America\u0026#39;s involvement in conflicts like Korea and Vietnam was seen by some as a way to contain communism and prevent its spread. In conclusion, while American military intervention has played an important role in shaping global politics, it is essential for policymakers to carefully consider the potential consequences of such actions. Balancing national interests with respect for international norms and human rights should be at the forefront of any decision to deploy U.S. forces abroad. The following are some key points that can help in understanding the complexities of American interventionism: Historical context: The United States has a long history of using its military power to influence global events, dating back to colonial times and continuing through various conflicts such as World Wars I and II, Korea, Vietnam, and more recent engagements like Afghanistan and Iraq. This historical perspective helps us understand the motivations behind U.S. interventionism and how it has evolved over time. Motivations: The reasons for American military interventions vary widely, ranging from protecting national security interests to promoting democracy and human rights. However, critics argue that these justifications are often used as a cover for pursuing economic or geopolitical goals. Understanding the motivations behind U.S. interventionism can help us evaluate its effectiveness and potential consequences. Consequences: The impact of American military interventions is complex, with both positive and negative outcomes. While some argue that interventions have helped promote democracy and protect human rights in certain cases, others point to the unintended consequences such as increased violence, political instability, and resentment towards U.S. forces among local populations. International norms: The United States\u0026#39; approach to military interventionism has often been criticized for not aligning with international norms and laws governing the use of force. This includes issues such as sovereignty, proportionality, and accountability in cases where civilian casualties occur during conflicts. Alternatives: Some scholars and policymakers advocate for alternative approaches to addressing global challenges that do not rely on military interventionism. These include diplomacy, economic development, and multilateral cooperation through international organizations like the United Nations. Evaluating these alternatives can help us better understand the role of U.S. interventionism in a broader context. In summary, American interventionism is a complex issue with both supporters and critics. While it has played an important role in shaping global politics, policymakers must carefully consider its potential consequences and explore alternative approaches to addressing international challenges. Q:나의 학과는 뭐임? A:안녕하세요, 제 이름은 John Doe입니다. 나의 학과는 컴퓨터인공지능학과입니다. [1] 과제에 관한 내용을 찾아보세요. [1] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 이 분야에서 나는 과제를 해보세요. [2] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5: \u0026#34;Vol. 48, No. 5, May 2 Written in the year 1930 by a young man named John Doe, this letter was found among his personal belongings after he passed away. The letter reveals insights into his life and thoughts during that time period.\u0026#34; 이렇게 생각했습니다. 1930년에 올해 생각한 소년이의 일상과 생각을 보여주는 이렇게 쓸 노약성적 편지를 찾아볼까요. [3] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 이 분야에서 1930년대의 일상과 생각을 보여주는 편지를 찾아볼까요. Written in the year 1930 by a young man named John Doe, this letter was found among his personal belongings after he passed away. The letter reveals insights into his life and thoughts during that time period. [4] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [5] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [6] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5: \u0026#34;Vol. 48, No. 5, May 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [7] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=48\u0026amp;number=5: \u0026#34;Vol. 48, No. 5, May 2 Written in the year 1930 by a young man named John Doe, this letter was found among his personal belongings after he passed away. The letter reveals insights into his life and thoughts during that time period.\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [8] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [9] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [10] URL: https://journal.kics.or.kr/journals/jkics/digital-library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [11] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [12] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [13] URL: https://journal.kics.oroe.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [14] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [15] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [16] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [17] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [18] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [19] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [20] URL: https://journal.kics.or.kr/journals/jkics/digital_library/publication?volume=47\u0026amp;number=6: \u0026#34;Vol. 47, No. 6, Jun. 2022\u0026#34; 이렇게 생각했습니다. 1930년대의 일상과 생각을 보여주는 이력을 찾아볼까요. [BONUS] LLM을 통해서 QUERY를 행당 QUERY를 잘 설명하는 압축 문장으로 요약하여 저장합니다.\nEXAMPLE:\nGEMINI: 나의 학과를 알려줘 → 학과 정보 나의 학과를 알려줘 → 학과 확인 Llama 3B instruct: 나의 학과를 알려줘 → 학과 Mistral: 나의 학과를 알려줘 → 학과 (Department) [시현] ","permalink":"https://new-sunset-shimmer.github.io/project_1/","summary":"공모전 2024","title":"RAG Project"},{"content":" Full Title : RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS\nLink : 2309.15531 (arxiv.org)\nRelated Link : johnheo/adadim-llm: [ICLR 2024] Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models (github.com)\nReleased Date : Wed Dec 4 2024\nReview Date : Wed, 27 Sep 2023\nPer Input Channel Quantization\nMotivation Authors find that Some layers not sensitive to output channel of weight, It is more related to Input channel that directly conduct by input. Authors propose Per-input-channel quantization. But Figure 2(left) even if same layers they are got different sensitive. Also Table 1 only apply Per-IC quant to attn.qkn and mlp.down layer improve overall quality. so authors propose ADAPTIVE PER-CHANNEL QUANTIZATION that find Per-IC or Per-OC. Research questions Is Quantize by channel is efficient? Methodology Authors quantize by Input channels in order to isolete outliers. Outliers not appear everywhere, it only presistent in fixed channels. If we generally quantize channels outlier make affect all input channels. ADAPTIVE PER-CHANNEL QUANTIZATION. as mentioned in Motivation not all layer is sensitive to Input channel and not all layer got huge magnituted named outlier. Authors propose determine oc and ic which method is more suit able for layer. Main result reorder option is make quantization make more good overall performance on quantization method base model is LLama accuracy on instructio-tuned LLama accuracy on Task-specific qunatization. target mean use calibrated dataset based on specific tast Perplexity based on method, group, bit. Using ada is make perplexity better Recinstruction error per index. Initial layers more sensitive to outliers but All layers got different sensitivity based on task, model size, model type Personal thought Layers are got different sensitive based on task, model size. Is it mean outliers appear different channel or not appear based on task, size ? ","permalink":"https://new-sunset-shimmer.github.io/RETHINKING/","summary":"Per Input Channel Quantization","title":"RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS"},{"content":"2407.08044 (arxiv.org)\nFull Title : RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization\nReleased Date : 2024 09 20\nReview Date : 2024 10 21\nRotation에 Lora를 적용시킨 모델\nMotivation LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 그 문제를 해결하기위해 quantazation을 실행하지만 성능이 크게 나오지 않는 문제가 존재한다. 이는 Outlier를 제대로 활용하지 못해서 생긴 문제다. 그를 해결하기 위해서 Rotatation layer를 사용한다. 이번 논문에서는 Rotation에 Lora를 적용하여 효과적으로 학습 및 빠른 계산을 논점으로 소개하는 논문이다. 해당 논문이 다른 경량화 논문과의 차이점은 멀티모델에 해당 방법론을 적용시켜 성능을 보존시켰다. Research questions Can we preserve the outlier-free characteristics of rotated LLMs and benefit from them during PEFT?\nPEFT를 적요시키면서 outlier의 장점을 그래도 가져갈수있나?\nMethodology 해당 논문에서는 Lora After Roatation[LRAR]로 Lora를 적용시켰다. Main result Llama2 , LLama 3을 기준으로 Alpaca를 기준으로 미세조정을 진행했다. Quantazation은 가장 많이 사용되는 GPTQ로 진행했다. 또한 왼쪽 : 학습시 activition에 발생하는 이상치, 중간 : 학습이 다 끝난뒤의 layer별 이상치, 오른쪽 : 레어별 양자화로 인해생기는 오류의 합 비전 모델을 오직 양자화만 하고 평가\nPersonal thought 어떻게 미세조정없이 양자화된 모델에 Lora만 적용시켜도 성능이 오르는가?\nLoRA로 rotation을 만들지는 못하는가?\n","permalink":"https://new-sunset-shimmer.github.io/RoLoRA/","summary":"Rotation에 Lora를 적용시킨 모델","title":"RoLoRA"},{"content":" Full Title : ROUND AND ROUND WE GO! WHAT MAKES ROTARY POSITIONAL ENCODINGS USEFUL?\nLink : 2410.06205 (.org)\nRelated Link : RoFormer (arxiv.org) RoPE (Rotary positional embeddings) explained: The positional workhorse of modern LLMs - YouTube\n[[논문리뷰] Enhanced Transformer with Rotary Position Embedding(RoFormer) (tistory.com)](https://g3lu.tistory.com/38) [[논문리뷰] RoFormer: Enhanced Transformer with Rotary Position Embedding - 전생했더니 인공지능이었던 건에 대하여 (kimjy99.github.io)](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/roformer/) [https://drive.google.com/file/d/152NPPyNjo-N6MMIaupXacS41BUJgjE5l/view](https://drive.google.com/file/d/152NPPyNjo-N6MMIaupXacS41BUJgjE5l/view) [[논문리뷰] RoFormer: Enhanced Transformer with Rotary Position Embedding (velog.io)](https://velog.io/@you-genie/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-RoFormer-Enhanced-Transformer-with-Rotary-Position-Embedding) Released Date : 2024 10 08\nReview Date : 2024 11 15\nRoPE에 대한 분석\nMotivation 이번논문에서는 RoPE(Rotary Positional Encodings)이 실제로 좋은 방법인지를 확인한다. Research questions RoPE는 효과적인 방법론이 맞는가?\nRoPE는 왜 효과적인가?\nNoPE는 왜 효과적인가?\nMethodology RoPE\n기존의 Attention은 Query와 Key의 내적으로 계산된다. 이때 Query와 Key의 상대적인 정보는 고려되지 않는다. 그렇기에 RoPE이전에도 Attention을 계산할때 많은 위치정보도 같이 주는 연구들이 있었다. 위의 그림에서 보다싶이 이전 연구들은 단순히 더해주거나 아니면 곱해주는 방식으로 접근했다. 이러한 방식은 원본값을 크게 아니면 적게, 다른 방향으로 다른 스케일로 변환되어 비효율적이다. - Rotary Positional Encodings은 특정한 값을 더해주거나 곱해주는게 아닌 현재해당 위치에 따라 매트릭스를 회전시켜준다. 가중치 W와 입력 X를 각각 cos, sin을 이용하여 회전시킨다. 여기서 delta는 Frequency, hyperparameter(식 4의 10000처럼)다. 위의 식을 다차원으로 확장하여 아래와 같이 서술한다. R은 아래와같이 유사 대각행렬로 표현할수있다.\n계상효율화를 위해서 elementwise연산을 사용할수있다.\n위의 RoPE는 현재 많이 쓰이고있다. 가장 훌륭한 예시로 LLama 모델이 해당 방법론을 사용하고있다. 하지만 이 방법론에도 단점이 존재하며 이번 논문에서는 해당 문제점을 다룬다. RoPE의 성질 Long-term decay. RoPE는 위치가 먼 token들간에 어텐션 score가 낮아진다. RoPE논문에서는 해당 성질은 먼 단어들은 서로 연관이 적다는 intuitaion과 일치한다고 서술했다.\n하지만 이번 논문에서는 Long-term Decay가 실제 상황에서는 이루워지지 않는다고 주장한다. Query랑 Key가 일정한 값 ‘all-ones’의 경우 거리에 따라 Decay가 발생하지만 Gaussian 분포도를 따르는 Query랑 Key를 임의로 생성하여 RoPE를 적용시킬경우 Decay 성질은 존재하지않는다.\nFrequencies가 어떻게 사용되고 있나? 아래의 그림을 보면 모델은 대부분의 레이어 에서 Low Frequencies에 대해 학습을 하고있다는걸 알수있다. 중간에서 high쪽으로는 값이0에 가까우며 유일하게 1레이어의 초기 토큰에 대한 정보에는 민감하다. 이는 내적(dot product)에 영향으로 이렇게 학습됐다고 논문에서는 서술한다. 다시 말해 sementic에 대한 정보를 구할때 회전이 적을수록 좋다. 하지만 특정한 헤드에서는 high frequence에 대한 정보는 학습한다. 아래에서 보면 해당 헤드에서는 위치에 대한 정보를 학습한다. 어텐션 패턴을 확인하면 이전 토큰, 아니면 지금 토큰의 위치에 집중한다. 이는 NoPE에서는 보이지 않는 패턴이다. 하지만 너무 low한 Frequencies에는 회전을 하지 않아 BOS(시작) 토큰에만 집중을한다. 그렇기에 Contex길이가 학습된 길이를 넘을 경우 제대로된 성능이 나오지 않는 것이다. 이에 대한 해결책으로 저자들은 R 매트릭스를 짜르는 제안을 했다. 0.75만 가지고 가거나 0.25만 가지고 가는거다. ","permalink":"https://new-sunset-shimmer.github.io/Round/","summary":"RoPE에 대한 분석","title":"ROUND AND ROUND WE GO! WHAT MAKES ROTARY\nPOSITIONAL ENCODINGS USEFUL?"},{"content":" 인턴용 리뷰\n발표 날짜 : 2024-07-03\n링크: 2401.15024 (arxiv.org)\n깃허브 링크 : microsoft/TransformerCompression: For releasing code related to compression methods for transformers, accompanying our publications (github.com)\nLLM 경량화 방법에 대한설명\n문제 LLM은 현재 자연어처리 분야의 가장 큰 근본이 되고있다. 하지마 LLM의 scaling-law으로 인해 나날이 사이즈가 늘어났다. 그로 인해 계산량 증가, 모델 사이즈가 증가하여 추가 저장장치 및 계산장치가 필요하다. 또한 많은 모델은 학습후에 sparse한 가중치를 가지고 있다. scaling-law은 결국 모델의 가중치를 scaling 하는 행위다. 그렇기에 사이즈를 줄이기위해서는 모델의 가중치에 변화를 줘야한다 제안 방법 SLICEGPT는 이러한 문제를 해결하는 pruning 방법이다. 하지만 저자들은 일반적인 pruning 방법은 가중치를 줄이기 위해서 0으로 thershhold 하는 방식은 sparsity를 증가시켜 성능저하가 심하다. 저자들은 Q 가중치를 만들어 선형대수학의 성질을 이용하여 계산상 Q가 pruning 되지 않은 결과가 나오는 방법을 제안했다. 아키텍쳐 기존 transformer 아키텍쳐에 SLICEGPT를 적용시키기 위해서 어느정도의 변화가 많이 필요하다.\nLayerNorm 변화:\nLayerNorm은 Q를 적용할때 계산식에 방해가된다. 그렇기에 M과 diag(epsilon)을 주위 Layer로 넘겨야한다. M은 activation 이후 가중치로 diag(epsilon)은 activition 이전의 가중치로 옮긴다. M은 평균을 뺴주는 역할을 하며 epsilon은 scaling을 위해서 존재하는 학습형 파라미터다\nQ 가중치 생성:\n해당 모델에 맞는 Q 가중치를 생성해야한다. Q를 계산하기 위해서 PCA를 이용해야한다. 학습 데이터 셋에서 특정 부분을 남기고 모델을 지나게 한다. Q는 C행렬의 고유값의 내림차 순이다.\nQTQ를 생성:\n구한 Q들을 기준으로 모델에 적용시켜야한다. 하지만 residual을 위해서 이전 값을 다음 layer의 출력에 더해야하는데 생으로 더할수 없기에 QTQ롤 곱해서 더해준다. ex) XQ * QTQ = XQ\n실험결과 python run_slicegpt.py \u0026ndash;model facebook/opt-125m \u0026ndash;save-dir dir/to/save/sliced_model/in \u0026ndash;sparsity 0.25 \u0026ndash;device cuda:0 \u0026ndash;eval-baseline \u0026ndash;no-wandb\n","permalink":"https://new-sunset-shimmer.github.io/slicegpt/","summary":"LLM 경량화 방법에 대한설명","title":"SLICEGPT"},{"content":" Full Title : SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\nLink : SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (arxiv.org)\nRelated Link : mit-han-lab/smoothquant: [ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (github.com)AWQ (notion.so)\nReleased Date : Fri, 18 Nov 2022\nReview Date : Web, Nov 20th 2024\nMake easy weight and activation to quantize\nMotivation LLM is hard to quantize because of outlier. outlier is huge magnitude values that appear fixed channels. it is make activation harder to quantize. in this paper we make x to make smoother so we can quantize easy. in order to that we split complexity of activation quantize to weight Research questions can we split complexity of activation to weight ? Methodology In general per-cahnnel quantization is very effective. but it is not compatible with GEMM kernels. Speed of Per-channel quantization is as same as FP16.\nIn this paper we need to find scaling factor s to smooth the values. but if we find by max(x), all of complexity migrate to weight. if we find by 1/max(w) activation is still hard to quantize. so we need to trade of between x and w. alpha is real number between 0-1.0. Alpha is can find between what is the important number to make both easy to quantize. eventually we make Activation to easy to quantization. Also easy to quantize by tensor\nSmoothquant use INT8 GEMM. Softmax and LayerNorm layer user FP16.\nMain result Smooth quant use per tensor. Per-tensor dynamic mean in online and static is mean pre compute the Scale Accuracy and ppl of evalution by lm_eval harness SmoothQuant works on various architectures\nPersonal thought If it work on 4W4A ? ","permalink":"https://new-sunset-shimmer.github.io/SmoothQuant/","summary":"Make easy weight and activation to quantize","title":"SmoothQuant"},{"content":" Full Title : SP3 : Enhancing Structured Pruning via PCA Projection\nLink : 2308.16475 (arxiv.org)\nRelated Link :\nReleased Date : Thu, 31 Aug 2023\nReview Date : Nov 2024\nprincipal components aware pruning\nMotivation Recently LLM has shown great performance on various task. But LLM got several cons. 1) Huge size 2) huge gpu consumption 3) Huge computation cost.\nStructured Pruning is widely used technique for reducing the size of pre-trained llm(in this paper calling pre trained language model(PLM)). But most of Structured pruning methods aren’t consider Principle components. In this paper authors make pruning to aware principle component to better structured pruning by PCA Projection.\nResearch questions Can we make Pruning to aware own matrices principle component? Methodology Author find Principal Components of Weight matrix is important. They are all across in index, all number of Principals are diffrent across all layer. Previous researchs just focusing on decrease weight dimension d to f that f\u0026lt;d not considering the principal components. - Below formulation is general pruning formulation. SP^3 got 3 step to prune a model - PCA Projection can expand general pruning formulation. Pin = U^TR Pout = diag(epsilon)U - in MHA we can expand like below Model Pruning\nSP^3 use pruning loss that forces the expected sparsity of the model to be close to the desired sparsity. t is target sparsity, s is expected sparsity - in this paper do multi step pruning. first dimension and add head, layer level mask Model Fusing\nAfter training merge mask with weight matrices. Pr denotes pruning all zero rows and columns of the matrix.\nMHA Layer Fusing - FFN Layer Fusing - Residual Modification Main result ","permalink":"https://new-sunset-shimmer.github.io/SP3/","summary":"principal components aware pruning","title":"SP3"},{"content":" Full Title : SpinQuant: LLM Quantization with Learned Rotations\nLink : 2405.16406 (arxiv.org)\nReleased Date : 2024 10 07\nReview Date : 2024 10 22\n학습가능한 Rotation matrix을 이용한 Quatazation\nMotivation PTQ는 따로 학습이 필요없이 모델의 사이즈와 계산량을 줄일수있는 장점이있다. 하지만 “Outlier”라는 중요한 벡터에 인해서 방법론에 인한 성능저하에 인해서 과연 효과적인지 의문을 가질수있다. 이번 논문은 Rotation matrix을 이용하여 outlier를 제어하여 Quantize에 보다 쉽게 만드는게 목표다. Research questions Rotation을 양자화의 성능을 최대로 만드는 최적화가 가능한가? Methodology 해당 논문에서는 R_1 R_2 R_3 R_4라는 다양한 Rotation matrix을 아키텍쳐에 적용시켰다. R_1과 R_2는 학습 가중치, R_3 R_4는 hadamard rotation matrix이다. Rotation은 외부에서 가중치에 전혀 흡수되지 않음으로 따로 Hadamard를 적용시킨거다. 해당 논문에서는 R_1, R_2를 무작위로 초기화하는게 가장 좋다고 판단했다. 하지만 아래의 그림에서 확인 가능하다싶이 그 Variance가 지나치게 높다 해당 문제를 해결하기 위해서 저자들은 Cayley-optimized rotation이라는 Rotation 전용 최적화 방법을 제안했다.\nStiefel 다양체는 정규 직교 행렬들이 구성하는 공간으로, 정규 직교 행렬의 모든 열이 서로 직교하며 각 열의 크기는 1입니다. 정규 직교 행렬은 회전이나 반사를 표현하는 데 사용되며, Cayley 변환은 이 행렬이 가질 수 있는 구조를 유지하면서 최적화를 수행할 수 있게 합니다.\n예를 들어, 양자화를 수행할 때 모델의 가중치나 활성화 값이 특정 축에 따라 비정상적으로 큰 값을 갖는 이상치가 있을 수 있습니다. Stiefel 다양체 상에서 회전 행렬을 최적화하면 이러한 이상치를 분산시켜 양자화 범위 안에 더 잘 맞추도록 만들어 줍니다. 이때 Stiefel 다양체의 정규 직교성을 유지해야 하기 때문에, 일반적인 최적화 방식이 아닌 특수한 방법이 필요하게 됩니다.\nCayley-optimized rotation의 역할\nCayley-optimized rotation을 통해 학습된 회전 행렬은 가중치와 활성화 값의 이상치를 줄이고 양자화 과정에서 발생하는 오류를 최소화할 수 있습니다. 예를 들어, 원래는 큰 값 하나 때문에 양자화가 비효율적으로 이루어질 수 있지만, Cayley-optimized rotation을 적용하면 각 축에 대한 분포가 보다 고르게 되어 양자화 효율성이 높아집니다.\n최적화 과정 요약 초기화: Cayley 변환을 이용해 회전 행렬을 초기화합니다. 초기 값으로는 일반적으로 Hadamard 행렬이나 무작위 직교 행렬을 사용합니다.\n회전 행렬 업데이트: Cayley 변환을 통해 그래디언트를 기반으로 회전 행렬을 업데이트하여 이상치가 줄어들도록 최적화합니다.\n양자화 수행: 최적화된 회전 행렬을 모델에 적용하여 양자화 범위 내에 더 많은 가중치와 활성화 값을 균등하게 분산시키고, 양자화 오류를 최소화합니다.\nMain result R_1, R_2를 무작위 hadamard martix으로서 초기화, GPTQ를 사용하여 quantization. SpinQuant no had는 R_1과 R_2만 사용 FP rotation과 Hadamard rotation의 차이 Personal thought Softmax층의 Rotation만 남겨도 성능이 좋을까?\nRotation을 다른 데이터 셋에 학습 시킬경우?\n","permalink":"https://new-sunset-shimmer.github.io/SpinQuant/","summary":"학습가능한 Rotation matrix을 이용한 Quatazation","title":"SpinQuant"},{"content":" Full Title : SpinQuant: LLM Quantization with Learned Rotations\nLink : 2405.16406 (arxiv.org)\nReleased Date : 2024 10 07\nReview Date : 2024 10 22\n학습가능한 Rotation matrix을 이용한 Quantization\nMotivation PTQ는 따로 학습이 필요없이 모델의 사이즈와 계산량을 줄일수있는 장점이있다. 하지만 “Outlier”라는 중요한 벡터에 인해서 방법론에 인한 성능저하에 인해서 과연 효과적인지 의문을 가질수있다. 이번 논문은 Rotation matrix을 이용하여 outlier를 제어하여 Quantize에 보다 쉽게 만드는게 목표다. Research questions Rotation을 양자화의 성능을 최대로 만드는 최적화가 가능한가? Methodology 해당 논문에서는 R_1 R_2 R_3 R_4라는 다양한 Rotation matrix을 아키텍쳐에 적용시켰다. R_1과 R_2는 학습 가중치, R_3 R_4는 hadamard rotation matrix이다. Rotation은 외부에서 가중치에 전혀 흡수되지 않음으로 따로 Hadamard를 적용시킨거다. 해당 논문에서는 R_1, R_2를 무작위로 초기화하는게 가장 좋다고 판단했다. 하지만 아래의 그림에서 확인 가능하다싶이 그 Variance가 지나치게 높다 해당 문제를 해결하기 위해서 저자들은 Cayley-optimized rotation이라는 Rotation 전용 최적화 방법을 제안했다.\nStiefel 다양체는 정규 직교 행렬들이 구성하는 공간으로, 정규 직교 행렬의 모든 열이 서로 직교하며 각 열의 크기는 1입니다. 정규 직교 행렬은 회전이나 반사를 표현하는 데 사용되며, Cayley 변환은 이 행렬이 가질 수 있는 구조를 유지하면서 최적화를 수행할 수 있게 합니다.\n예를 들어, 양자화를 수행할 때 모델의 가중치나 활성화 값이 특정 축에 따라 비정상적으로 큰 값을 갖는 이상치가 있을 수 있습니다. Stiefel 다양체 상에서 회전 행렬을 최적화하면 이러한 이상치를 분산시켜 양자화 범위 안에 더 잘 맞추도록 만들어 줍니다. 이때 Stiefel 다양체의 정규 직교성을 유지해야 하기 때문에, 일반적인 최적화 방식이 아닌 특수한 방법이 필요하게 됩니다.\nCayley-optimized rotation의 역할\nCayley-optimized rotation을 통해 학습된 회전 행렬은 가중치와 활성화 값의 이상치를 줄이고 양자화 과정에서 발생하는 오류를 최소화할 수 있습니다. 예를 들어, 원래는 큰 값 하나 때문에 양자화가 비효율적으로 이루어질 수 있지만, Cayley-optimized rotation을 적용하면 각 축에 대한 분포가 보다 고르게 되어 양자화 효율성이 높아집니다.\n최적화 과정 요약 초기화: Cayley 변환을 이용해 회전 행렬을 초기화합니다. 초기 값으로는 일반적으로 Hadamard 행렬이나 무작위 직교 행렬을 사용합니다.\n회전 행렬 업데이트: Cayley 변환을 통해 그래디언트를 기반으로 회전 행렬을 업데이트하여 이상치가 줄어들도록 최적화합니다.\n양자화 수행: 최적화된 회전 행렬을 모델에 적용하여 양자화 범위 내에 더 많은 가중치와 활성화 값을 균등하게 분산시키고, 양자화 오류를 최소화합니다.\nMain result R_1, R_2를 무작위 hadamard martix으로서 초기화, GPTQ를 사용하여 quantization. SpinQuant no had는 R_1과 R_2만 사용 FP rotation과 Hadamard rotation의 차이 Personal thought Softmax층의 Rotation만 남겨도 성능이 좋을까?\nRotation을 다른 데이터 셋에 학습 시킬경우?\n","permalink":"https://new-sunset-shimmer.github.io/SpinQuant/","summary":"학습가능한 Rotation matrix을 이용한 Quantization","title":"SpinQuant"},{"content":" Full Title : SqueezeLLM: Dense-and-Sparse Quantization\nLink : SqueezeLLM: Dense-and-Sparse Quantization (arxiv.org)\nRelated Link : SqueezeAILab/SqueezeLLM: [ICML 2024] SqueezeLLM: Dense-and-Sparse Quantization (github.com)\nReleased Date : Tue, 13 Jun 2023\nReview Date : Sun Nov 24 2024\nExclude Dense and Sparse vector\nMotivation Existing LLM is very huge and unfavor to frame of devices. Quantazation is state-of-art approach to solve this problem but tradeoff speed and accuracy. Also, Authors mention most of speed degrade came from memory reading, computation speed is marginal. In this paper authors propose two novel ideas. First sensitivity based non-uniform quantization, Second The Dense-and-Sparse decomposition. Research questions Is only computation cost is degrade model speed?\nIs there any weight that important in model?\nMethodology Sensitivity-Based Non-uniform Quantization\nNon-uniform Quantization is more accuracy than Uniform Quantization because most Weight matrix isn’t uniform, Also uniform quantization isn’t superior at speed than Non-uniform quantization. So, Authors select Non-uniform quantization - Finding a sensitive value is “not” easy. Personally it is near close to NP hard. Authors shift problem to K-means problem on given the model weight. - According to equation 1, We quantize weight(W) based on k clusters. Quantized weights(w) represented by k distinct values {q1, …, qk}. Q(w)* can be obtained by 1-dimentional k-means clustering. Centriod of each cluster as qj - Authors want to minimize the overall perturbation with respect to the final loss rather than focusing on individual layers. It is mean authors not minimize problem of finding sensitive value. For better(Minimize) final loss Centroid(qj) is need to be in sensitive value - Expanding equation 1 by Tayler series we can get equation 2 and equation 3. if Loss is 0 we can ignore gradient[g] and L(w). So we can obtain equation 4. - Second derivative, H(Hessian matrix) is more sensitive to small perturbations. - Hessian matrix is got huge computing cost, Authors approximate the hessian matrix based on Fisher information - Training by equation 6 centroids are close to Top-20 sensitive values. Dense-and-Sparse Quantization\nOutlier is challenge in low-bit LLM. In this paper removing small percentage(0.1%) of values that outlier keep as FP16 and make them to another matrix(named Sparse matrix). Dense matrix is matrix that not containing outliers.\nAuthors find outliers by Threshold values. W can make by sum of D,S. D = W[Tmin \u0026lt; w \u0026lt; Tmax], S = W[w \u0026lt; Tmin or w \u0026gt; Tmax]. S value is very small percentage that negligible in speed and memory load. WX = DX + SX. If we mitigate to Sparse matrix, we can better quantize the dense matrix.\nSensitivity-Based Sparse Matrix\nFor more better accuracy authors isolate sensitive values(it is easily identified based on Fisher information) in FP16. Main result Use GPTQ with activation ordering\nPerformance of SqueezeLLM, SqueezeLLM is Dense-only Quantization(no exclude outliers). Dense-and-Sparse Quantization(using Dense matrix)\n3-bit quantization where even dense-only SqueezeLLM achieves comparable perplexity to grouped GPTQ/AWQ. 0-shot MMLU evalution Speed and memory usage Instruction-Following Ability Personal thought How Top 20 Sensitive values are computed in Figure 3? ","permalink":"https://new-sunset-shimmer.github.io/SqueezeLLM/","summary":"Exclude Dense and Sparse vector","title":"SqueezeLLM"},{"content":"2405.17927 (arxiv.org)\nThe Evolution of Multimodal Model Architectures\nReleased : 2024 05 28\nReview Date : 2024 09 21\n멀티 모델들의 아키텍쳐 분류\nMotivation 저자들은 현재 연구되고있는 멀티모델들을 조사했으며 그 모델들을 분류하고 그에 따른 데이트셋과 특징들을 해당 논문에서 설명했다. Methodology 저자들은 모델 분류를 총 A,B,C,D 4개로 분류했다. A와 B는 모델 내부에 파라미터를 추가하는 Deep fusion, C와 D는 입력층에 파라미터를 추가하는 Early fusion이다. Type A : encoder를 통해 LLM에 일반적인 cross-attention Layer로 상호작용한다. 보통 self-Attention뒤 혹은 앞에 넣는지로 갈린다.\n장점 : 모델의 흐름을 미세하게 조절이 가능. end-to-end 학습과 출력가능\n단점 : 필요 학습 데이터, 모델 사이즈 증가, 사용하는 LLM의 구조에대한 이해도가 필요. cross-attention Layer추가 후 다른 Layer 추가가 힘듬. 스케일링이 어려움\nType B : 일반적인 cross-attention이 아닌 Custom Cross-attention Layer, Custom Learnable Layer을 쓴다. Custom Cross-attention Layer은 일반적으로 encoder을 쓰며 Custom Learnable Layer은 굳이 encoder을 필요로 하지않는다.\n장점 : 모델의 흐름을 미세하게 조절이 가능. end-to-end 학습과 출력가능. A type과 비교하여 다른 Layer 추가가 비교적 쉬움. 파라미터 조절이 쉬움\n단점 : 스케일링에 어려움이 존재. 필요 학습 데이터, 모델 사이즈 증가. text 아닌 다른 형식의 출력을 auto-regressive하게 학습하기 힘듬\nType C : 모델 내부에 어떤 값을 보내지 않으며 입력층을 통해서 값을 모델로 전달한다. 보통 encoder층을 이용하고 해당 출력을 특정한 길이의 값으로 바꾸기 위해 특정한 Layer을 쓴다. LLM 아키텍쳐에 변화를 주지않는다(Lora는 쓸수있음). Encoder 출력을 바꾸는 Layer에는 Reasmpler(Q-former과 비슷하지만 image와 text는 다른 self-attention층을 씀), Linear Layer, MLP, Q-former(가벼운 transformer. 텍스트, 이미지를 학습하는 하나의 query embeddings, cross-attention을 가지고있으며 이미지와 텍스트는 하나의 self-attention층을 공유), Attention Pooling등이 있다.\n장점 : 필요 학습 데이터, 모델 사이즈 증가가 비교적 적음. end-to-end 학습과 출력가능. LLM을 쉽게 바꿀수있음. 모델 타입의 모델 사용가능. Layer추가가 비교적 쉬움.\n단점 : 모델의 흐름을 미세하게 조절이 힘듬. 텍스트가 출력으로 auto-regressive하게 학습이 힘듬\nType D : 이미지, 비디오, 오디오를 특정한 토큰들로 변환하는 타입이다. LLM, Encder-Decder모델을 쓰는 형식으로 나뉜다.\n장점 : 모든 형식의 출력에 auto-regressive하게 학습이 가능. end-to-end 학습과 출력가능.\n단점 : 모델의 흐름을 미세하게 조절이 힘듬. 학습해야 하는 파라미터가 다른 타입들 보다 많음(다른 Token들을 학습해야 하기에)\n","permalink":"https://new-sunset-shimmer.github.io/Multimodal/","summary":"멀티 모델들의 아키텍쳐 분류","title":"The Evolution of Multimodal Model Architectures"},{"content":"Notion Database Duplicate the template in your workspace :\nhttps://malsius.notion.site/malsius/3602f007cbae4b76a4998a78caba0079?v=548ff3a915134267960b702e4b70047a\nAPI integration Go to your integrations in Notion and create a new integration, and name it\nChoose the workspace in which you have duplicate the template\nSet it capabilities like above, indeed the integration only need read access\nGet the API key and save it, you will need it later\nAllow integration to access your database Go to your notion’s blog database and add your integration to it\nGitHub Create a new repository Create a new repository using this template\nWhen you choose the repository name you have two options :\nif you choose \u0026lt;github_username\u0026gt;.github.io your blog’s URL will be https://\u0026lt;github_username\u0026gt;.github.io\nif you choose anything else your blog’s URL will be https://\u0026lt;github_username\u0026gt;.github.io/\u0026lt;repository_name\u0026gt; or you will be able to set your own domain name later if you have one\nEdit Hugo config file and GitHub Actions file Depending on your blog’s domain, edit the config.yml file on the root of your repository, change baseURL at line 1\nIn the previous step, if you use URL like https://\u0026lt;github_username\u0026gt;.github.io/\u0026lt;repository_name\u0026gt; you need to edit wrapper parameters in .github/workflows/build.yml and change /notion-ghpages with /\u0026lt;repository_name\u0026gt; or delete it if you use your own domain name or if you use URL like https://\u0026lt;github_username\u0026gt;.github.io\nIf you want to use your own domain name, add cname: \u0026lt;your_domain\u0026gt; at the end of .github/workflows/build.yml like above :\nAdd GitHub Actions secrets In the settings of your repository, you need to add two variables to allow GitHub Actions to fetch your posts on Notion\nNOTION_API_KEY : your Notion integration API key\nNOTION_DB_ID : ID of your Notion’s blog database like that :\nGitHub Actions permissions access You need to give write access to GitHub Actions to allow it to create a branch for your blog\nBuild your blog When you want to build your blog or just publish an update you need to the following :\nDeploy your blog If you use your own domain name you need to set it\nAnd add these A entries in your DNS record :\n185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 You can find further information here\nTroubleshooting GitHub uses CDN caching for Pages, so when you made an update it can take about 10 minutes to appear on your blog. As workaround you can clear your browser’s cache then refresh your page.\nIf the build fail, it can be due to unsupported Notion block on the wrapper, so you can open an issue here or it can be due to a missing properties on your Notion database, so you can check the changelog file here to update it.\n","permalink":"https://new-sunset-shimmer.github.io/usage/","summary":"Step-by-step guide to make your own blog with Notion and Hugo, hosted on GitHub Pages","title":"How to use"},{"content":"List of compatible Notion’s blocks :\nHeading 1 Heading 2 Heading 3 Paragraph\nTo-do 1\nTo-do 1.1 To-do 2\nBulleted list 1\nBulleted list 1.1 Bulleted list 2\nNumbered list 1\nNumbered list 1.1 Numbered list 2\nQuote\nprint(\u0026#34;Here a sample of Python code\u0026#34;) for i in range(42): print(i) This is an example of inline annotations with link, this can be also applied to headings.\n","permalink":"https://new-sunset-shimmer.github.io/features/","summary":"Example of post written with Notion","title":"Features"}]