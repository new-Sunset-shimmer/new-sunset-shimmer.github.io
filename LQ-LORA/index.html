<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LQ-LORA | Sunset</title>
<meta name="keywords" content="Quantization, Paper">
<meta name="description" content="Quantize Lora for efficient model finetuning">
<meta name="author" content="">
<link rel="canonical" href="https://new-sunset-shimmer.github.io/LQ-LORA/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://new-sunset-shimmer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://new-sunset-shimmer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://new-sunset-shimmer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://new-sunset-shimmer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://new-sunset-shimmer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://new-sunset-shimmer.github.io/LQ-LORA/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><style>
    @media screen and (min-width: 769px) {

         
        .post-content input[type="checkbox"]:checked~label>img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style><meta property="og:title" content="LQ-LORA" />
<meta property="og:description" content="Quantize Lora for efficient model finetuning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://new-sunset-shimmer.github.io/LQ-LORA/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LQ-LORA"/>
<meta name="twitter:description" content="Quantize Lora for efficient model finetuning"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://new-sunset-shimmer.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LQ-LORA",
      "item": "https://new-sunset-shimmer.github.io/LQ-LORA/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LQ-LORA",
  "name": "LQ-LORA",
  "description": "Quantize Lora for efficient model finetuning",
  "keywords": [
    "Quantization", "Paper"
  ],
  "articleBody": " Full Title : LQ-LORA: LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION FOR EFFICIENT LANGUAGE MODEL FINETUNING\nLink : 2311.12023 (arxiv.org)\nRelated Link : https://github.com/HanGuo97/lq-lora, 2305.14314 (arxiv.org), 1009.5055 (arxiv.org), 1010.2955 (arxiv.org), 0912.3599 (arxiv.org), 2110.05649 (arxiv.org)\nReleased Date : Mon, 20 Nov 2023\nReview Date : Thu Nov 28 2024\nQuantize Lora for efficient model finetuning\nMotivation There is plenty research on adding Low-rank adaptation at quantized weight for efficiency and speed. But Quantized q(W) + AB isn’t equal orignal W. In this paper update low-rank updates quantized W(q(W)) with quantization error. Inoder to do that, Authors decompose W into Q + L1 L2, train L1 L2 as Low-rank adaptation Research questions How to update quantized weight by Lora Methodology NormalFloat (NF): NF is for Quantile Quantization\nquantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. - Quantile Quantization : As mentioned above, quantile is divide distribution range into continuous equal intervals. Quantile quantization is quantize value to divided quantiles(also quantization bin). in one phrase we quantize by distribution. Example if NF4 we generate [-1.0,-0.8667,-0.7333,-0.6,-0.4667,-0.3333,-0.2,-0.0667,0.667,0.2,0.3333,0.4667,0.6,0.7333,0.8667,1.0], and input 0.5 is can be quantized to 0.4667. - But this is very slow, so use faster quantile like SRAM quantiles. Inorder to fast quantiles we need go fix expected values, variance. in QLoRA paper refrence Transformer model’s expected value is 0, make k-bit quantiles between [-1,1]. Authors divide matrix into specific number of blocks. As example d X k matrix into dk/B (B is block size).\nLQ-LORA\nLOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION\nIn LoRA reparamatereizes a pretrained matrix into W + L1 L2. They initializes L1 from a Gaussian, L2 to 0. It is ensure that L1 * L2 = 0 so model can finetune from zero(original distribution). But in quantized weight it is not very effective. Especially in low bit quantization error is ||W - Quantized(W)|| » 0. Authors propose Low-rank parameters need to be initialized by structure of Weight(w).\nHere is General loss function(1) between W and quantized Q + low-ranks. Q_b is mean NF-quantizable to b-bits. As authors. this optimization problem is similar to the one faced in robust principal components analysis. Authors solve it by approximately via altenating between optimizing L1L2, Q. Drive from equation 1. we can obtain new equation. Q^0 is intialized to 0. Above equation is found by heuristic. By Algorithm 2 we find optimal Q,L1,L2. Because authors use randomized SVD(for better speed improvements). simply check which loss is higher than previous time step’s L1 and L2, Q. Also optimizing RPCA via iteration expriments on loss values on different equetions. - **Robust principal component analysis** - Randomized SVD - MIXED-CONFIGURATION QUANTIZATION VIA AN INTEGER LINEAR PROGRAM For better store authors choose double quantizetion Quantize-NF is model quantization. Qunatize-INT is quantize the quantiles into integers. Quantiles got very small numbers it is almost lossless. - Dynamic quantization configurations. Prior works applying same quantization method to all matrix. All matrices aren’t got same value, same distribution also some of them is much harder than others. In this paper applying different configure to each matrix. - DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD mat Minimizing assignment matrix X that minimizes the Frobenius norm between the matrices before and after low-rank plus quantized decomposition, while respecting a target memory budget. for better minimize quickly authors use API [https://www.gurobi.com](https://www.gurobi.com/) DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD\nFollowing recent works Authors use Fisher information\nFrom calibrate dataset(d numbers) calculate derviation. Fisher information contain sensitivity of data and importance. Generally use Hadamard product. can apply to finding for L1, L2. equation above is np-hard. So authors relax problem to multiplying Column, Row of Fisher information. It is also can solve by standard svd Main result Finetuning of LQ-LoRA Quantized LoRA GLUE with RoBERTa-Large Personal thought Why is not Fisher information general? ",
  "wordCount" : "641",
  "inLanguage": "en",
  "datePublished": "2024-12-12T00:00:00Z",
  "dateModified": "2024-12-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://new-sunset-shimmer.github.io/LQ-LORA/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunset",
    "logo": {
      "@type": "ImageObject",
      "url": "https://new-sunset-shimmer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://new-sunset-shimmer.github.io/" accesskey="h" title="Sunset (Alt + H)">Sunset</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://new-sunset-shimmer.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      LQ-LORA
    </h1>
    <div class="post-meta"><span title='2024-12-12 00:00:00 +0000 UTC'>December 12, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#research-questions" aria-label="Research questions">Research questions</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a></li>
                <li>
                    <a href="#main-result" aria-label="Main result">Main result</a></li>
                <li>
                    <a href="#personal-thought" aria-label="Personal thought">Personal thought</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Full Title : LQ-LORA: LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION FOR EFFICIENT LANGUAGE MODEL FINETUNING</p>
</blockquote>
<blockquote>
<p>Link : <a href="https://arxiv.org/pdf/2311.12023">2311.12023 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Related Link : <a href="https://github.com/HanGuo97/lq-lora">https://github.com/HanGuo97/lq-lora</a>, <a href="https://arxiv.org/pdf/2305.14314">2305.14314 (arxiv.org)</a>, <a href="https://arxiv.org/pdf/1009.5055">1009.5055 (arxiv.org)</a>, <a href="https://arxiv.org/pdf/1010.2955">1010.2955 (arxiv.org)</a>, <a href="https://arxiv.org/pdf/0912.3599">0912.3599 (arxiv.org)</a>, <a href="https://arxiv.org/pdf/2110.05649">2110.05649 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Released Date :  Mon, 20 Nov 2023</p>
</blockquote>
<blockquote>
<p>Review Date : Thu Nov 28 2024</p>
</blockquote>
<p>Quantize Lora for efficient model finetuning</p>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>There is plenty research on adding Low-rank adaptation at quantized weight for efficiency and speed. But Quantized q(W) + AB isn’t equal orignal W. In this paper update low-rank updates quantized W(q(W)) with quantization error. Inoder to do that, Authors decompose W into Q + L1 L2, train L1 L2 as Low-rank adaptation</li>
</ul>
<h3 id="research-questions">Research questions<a hidden class="anchor" aria-hidden="true" href="#research-questions">#</a></h3>
<ul>
<li>How to update quantized weight by Lora</li>
</ul>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<ul>
<li>
<p>NormalFloat (NF): NF is for Quantile Quantization</p>
<ul>
<li><strong>quantiles</strong> are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way.</li>
</ul>
</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-650ea" hidden>
<label for="zoomCheck-650ea">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/55111ce3-bae5-4ab2-81fe-ce301b760be3.png#center" alt=""  />
</label></p>
<pre><code>- Quantile Quantization : As mentioned above, quantile is divide distribution range into continuous equal intervals. Quantile quantization is quantize value to divided quantiles(also quantization bin). in one phrase we quantize by distribution. Example if NF4 we generate [-1.0,-0.8667,-0.7333,-0.6,-0.4667,-0.3333,-0.2,-0.0667,0.667,0.2,0.3333,0.4667,0.6,0.7333,0.8667,1.0], and input 0.5 is can be quantized to 0.4667. 

- But this is very slow, so use faster quantile like SRAM quantiles. Inorder to fast quantiles we need go fix expected values, variance. in QLoRA paper refrence Transformer model’s expected value is 0, make k-bit quantiles between [-1,1].
</code></pre>
<ul>
<li>
<p>Authors divide matrix into specific number of blocks. As example d X k matrix into dk/B (B is block size).</p>
</li>
<li>
<p>LQ-LORA</p>
<ul>
<li>
<p>LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION</p>
<p>In LoRA reparamatereizes a pretrained matrix into W + L1 L2. They initializes L1 from a Gaussian, L2 to 0. It is ensure that L1 * L2 = 0 so model can finetune from zero(original distribution). But in quantized weight it is not very effective. Especially in low bit quantization error is ||W - Quantized(W)|| &raquo; 0. Authors propose Low-rank parameters need to be initialized by structure of Weight(w).</p>
</li>
</ul>
</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-ad06a" hidden>
<label for="zoomCheck-ad06a">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/20ca90c1-e999-41cc-8713-8f0dc7327ed7.png#center" alt=""  />
</label></p>
<pre><code>	Here is General  loss function(1) between W and quantized Q + low-ranks. Q_b is mean NF-quantizable to b-bits. As authors. this optimization problem is similar to the one faced in robust principal components analysis. Authors solve it by approximately via altenating between optimizing L1L2, Q.
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-21667" hidden>
<label for="zoomCheck-21667">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/7e1f5d02-342b-46a4-95b0-5b5530d4e216.png#center" alt=""  />
</label></p>
<pre><code>	Drive from equation 1. we can obtain new equation. Q^0 is intialized to 0. Above equation is found by heuristic. 

	By Algorithm 2 we find optimal Q,L1,L2. Because authors use randomized SVD(for better speed improvements). simply check which loss is higher than previous time step’s L1 and L2, Q. Also optimizing RPCA via iteration
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-13d37" hidden>
<label for="zoomCheck-13d37">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/02c45a37-9d3d-4e6b-be98-a74b8ec849f4.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-cf57a" hidden>
<label for="zoomCheck-cf57a">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/0dfd54db-48ac-4487-a0f1-d82b7df5bcf3.png#center" alt=""  />
</label></p>
<pre><code>	expriments on loss values on different equetions.
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-9f54d" hidden>
<label for="zoomCheck-9f54d">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/38ece6f5-2cab-4413-9615-3f0121f8e24e.png#center" alt=""  />
</label></p>
<pre><code>	- **Robust principal component analysis**
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-e3a2c" hidden>
<label for="zoomCheck-e3a2c">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/76d39750-938a-4d89-8da2-5d48cd383e15.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-fc2e6" hidden>
<label for="zoomCheck-fc2e6">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/cb559e64-1709-4aa9-8abb-f48e947b3152.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-0b405" hidden>
<label for="zoomCheck-0b405">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/12a40311-b61e-4e7c-9217-cac3341bc1e9.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-4673b" hidden>
<label for="zoomCheck-4673b">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/da6cee44-2b96-4ee7-963e-97844a175ea9.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-a47b3" hidden>
<label for="zoomCheck-a47b3">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/7b16a399-650c-410f-995f-52c3260447b8.png#center" alt=""  />
</label></p>
<pre><code>	- Randomized SVD
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-840ab" hidden>
<label for="zoomCheck-840ab">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/30c88a3d-d01b-4519-9997-99906d191889.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-7430f" hidden>
<label for="zoomCheck-7430f">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/e5e302af-35b5-45bf-9de1-da3058cd7e18.png#center" alt=""  />
</label></p>
<pre><code>- MIXED-CONFIGURATION QUANTIZATION VIA AN INTEGER LINEAR PROGRAM

	For better store authors choose double quantizetion
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-36d5b" hidden>
<label for="zoomCheck-36d5b">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/bea3f2d1-6e28-40cf-8b6b-828076caa307.png#center" alt=""  />
</label></p>
<pre><code>	Quantize-NF is model quantization. Qunatize-INT is quantize the quantiles into integers. Quantiles got very small numbers it is almost lossless. 
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-e3764" hidden>
<label for="zoomCheck-e3764">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/c95d3d60-7a47-462c-ad98-58e43b91cbf5.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-a3088" hidden>
<label for="zoomCheck-a3088">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/bbffae7a-630e-45ce-aa3b-f29bd9087bec.png#center" alt=""  />
</label></p>
<pre><code>	-  Dynamic quantization configurations.

		Prior works applying same quantization method to all matrix. All matrices aren’t got same value, same distribution also some of them is much harder than others. In this paper applying different configure to each matrix.

- DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD mat
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-e4766" hidden>
<label for="zoomCheck-e4766">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/8cfe78b1-bd20-4073-b88f-d0689fc70956.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-7f602" hidden>
<label for="zoomCheck-7f602">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/37523197-e000-4da7-8e18-937ce88af48f.png#center" alt=""  />
</label></p>
<pre><code>Minimizing assignment matrix X that minimizes the Frobenius norm between the matrices before and after low-rank plus quantized decomposition, while respecting a target memory budget.

for better minimize quickly authors use API [https://www.gurobi.com](https://www.gurobi.com/)
</code></pre>
<ul>
<li>
<p>DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD</p>
<p>Following recent works Authors use Fisher information</p>
</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-ac573" hidden>
<label for="zoomCheck-ac573">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/19cb7dd2-dec8-47a2-b333-309628ddd5de.png#center" alt=""  />
</label></p>
<pre><code>From calibrate dataset(d numbers) calculate derviation. Fisher information contain sensitivity of data and importance.
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-e2d74" hidden>
<label for="zoomCheck-e2d74">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/1134e61f-5c2e-4d1c-8b57-917ac61616c9.png#center" alt=""  />
</label></p>
<pre><code>Generally use Hadamard product.

can apply to finding for L1, L2.
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-59178" hidden>
<label for="zoomCheck-59178">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/21bbd747-28fe-488c-9718-37656cc9e9b8.png#center" alt=""  />
</label></p>
<pre><code>equation above is np-hard. So authors relax problem to multiplying Column, Row of Fisher information.
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-06a01" hidden>
<label for="zoomCheck-06a01">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/6030e1da-ac0a-4f38-8c05-173cb1b87c23.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-06151" hidden>
<label for="zoomCheck-06151">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/4cf496a1-c4a9-4d2c-9a63-2c34c7eb112e.png#center" alt=""  />
</label></p>
<pre><code>It is also can solve by standard svd
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-b66e8" hidden>
<label for="zoomCheck-b66e8">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/a9ee4c76-e1c7-4b4f-8e43-810f841a5b1d.png#center" alt=""  />
</label></p>
<h3 id="main-result">Main result<a hidden class="anchor" aria-hidden="true" href="#main-result">#</a></h3>
<ul>
<li>Finetuning of LQ-LoRA</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-8f915" hidden>
<label for="zoomCheck-8f915">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/95dd39b4-cae5-401e-a980-83bf62af2165.png#center" alt=""  />
</label></p>
<ul>
<li>Quantized LoRA</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-cedfb" hidden>
<label for="zoomCheck-cedfb">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/d503ebb5-63dc-43ee-90db-67b4c9895a7b.png#center" alt=""  />
</label></p>
<ul>
<li>GLUE with RoBERTa-Large</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-b80f5" hidden>
<label for="zoomCheck-b80f5">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/45324b64-fe38-48ed-9121-ea3f111f536d.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-fcbc9" hidden>
<label for="zoomCheck-fcbc9">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/02c26d36-9b31-4ddf-b47c-5322ac6415fa.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-55ce8" hidden>
<label for="zoomCheck-55ce8">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/7e9aa48f-0964-4a41-9c7c-20321b6f1461.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-ccf0b" hidden>
<label for="zoomCheck-ccf0b">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/1a5f3274-5205-4b05-9ec2-e8901213bdda.png#center" alt=""  />
</label></p>
<h3 id="personal-thought">Personal thought<a hidden class="anchor" aria-hidden="true" href="#personal-thought">#</a></h3>
<ul>
<li>Why is not Fisher information general?</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://new-sunset-shimmer.github.io/tags/quantization/">Quantization</a></li>
      <li><a href="https://new-sunset-shimmer.github.io/tags/paper/">Paper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://new-sunset-shimmer.github.io/">Sunset</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
