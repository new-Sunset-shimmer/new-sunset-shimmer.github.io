<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS | Sunset</title>
<meta name="keywords" content="Quantization, Paper">
<meta name="description" content="Per Input Channel Quantization">
<meta name="author" content="">
<link rel="canonical" href="https://new-sunset-shimmer.github.io/RETHINKING/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://new-sunset-shimmer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://new-sunset-shimmer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://new-sunset-shimmer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://new-sunset-shimmer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://new-sunset-shimmer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://new-sunset-shimmer.github.io/RETHINKING/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><style>
    @media screen and (min-width: 769px) {

         
        .post-content input[type="checkbox"]:checked~label>img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style><meta property="og:title" content="RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS" />
<meta property="og:description" content="Per Input Channel Quantization" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://new-sunset-shimmer.github.io/RETHINKING/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS"/>
<meta name="twitter:description" content="Per Input Channel Quantization"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://new-sunset-shimmer.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS",
      "item": "https://new-sunset-shimmer.github.io/RETHINKING/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS",
  "name": "RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS",
  "description": "Per Input Channel Quantization",
  "keywords": [
    "Quantization", "Paper"
  ],
  "articleBody": " Full Title : RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS\nLink : 2309.15531 (arxiv.org)\nRelated Link : johnheo/adadim-llm: [ICLR 2024] Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models (github.com)\nReleased Date : Wed Dec 4 2024\nReview Date : Wed, 27 Sep 2023\nPer Input Channel Quantization\nMotivation Authors find that Some layers not sensitive to output channel of weight, It is more related to Input channel that directly conduct by input. Authors propose Per-input-channel quantization. But Figure 2(left) even if same layers they are got different sensitive. Also Table 1 only apply Per-IC quant to attn.qkn and mlp.down layer improve overall quality. so authors propose ADAPTIVE PER-CHANNEL QUANTIZATION that find Per-IC or Per-OC. Research questions Is Quantize by channel is efficient? Methodology Authors quantize by Input channels in order to isolete outliers. Outliers not appear everywhere, it only presistent in fixed channels. If we generally quantize channels outlier make affect all input channels. ADAPTIVE PER-CHANNEL QUANTIZATION. as mentioned in Motivation not all layer is sensitive to Input channel and not all layer got huge magnituted named outlier. Authors propose determine oc and ic which method is more suit able for layer. Main result reorder option is make quantization make more good overall performance on quantization method base model is LLama accuracy on instructio-tuned LLama accuracy on Task-specific qunatization. target mean use calibrated dataset based on specific tast Perplexity based on method, group, bit. Using ada is make perplexity better Recinstruction error per index. Initial layers more sensitive to outliers but All layers got different sensitivity based on task, model size, model type Personal thought Layers are got different sensitive based on task, model size. Is it mean outliers appear different channel or not appear based on task, size ? ",
  "wordCount" : "303",
  "inLanguage": "en",
  "datePublished": "2024-12-12T00:00:00Z",
  "dateModified": "2024-12-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://new-sunset-shimmer.github.io/RETHINKING/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunset",
    "logo": {
      "@type": "ImageObject",
      "url": "https://new-sunset-shimmer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://new-sunset-shimmer.github.io/" accesskey="h" title="Sunset (Alt + H)">Sunset</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://new-sunset-shimmer.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS
    </h1>
    <div class="post-meta"><span title='2024-12-12 00:00:00 +0000 UTC'>December 12, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#research-questions" aria-label="Research questions">Research questions</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a></li>
                <li>
                    <a href="#main-result" aria-label="Main result">Main result</a></li>
                <li>
                    <a href="#personal-thought" aria-label="Personal thought">Personal thought</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Full Title : RETHINKING CHANNEL DIMENSIONS TO ISOLATE OUTLIERS FOR LOW-BIT WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS</p>
</blockquote>
<blockquote>
<p>Link : <a href="https://arxiv.org/pdf/2309.15531">2309.15531 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Related Link : <a href="https://github.com/johnheo/adadim-llm">johnheo/adadim-llm: [ICLR 2024] Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models (github.com)</a></p>
</blockquote>
<blockquote>
<p>Released Date :  Wed Dec 4 2024</p>
</blockquote>
<blockquote>
<p>Review Date : Wed, 27 Sep 2023</p>
</blockquote>
<p>Per Input Channel Quantization</p>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>Authors find that Some layers not sensitive to output channel of weight, It is more related to Input channel that directly conduct by input. Authors propose Per-input-channel quantization. But Figure 2(left) even if same layers they are got different sensitive. Also Table 1 only apply Per-IC quant to attn.qkn and mlp.down layer improve overall quality. so authors propose ADAPTIVE PER-CHANNEL QUANTIZATION that find Per-IC or Per-OC.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-070c5" hidden>
<label for="zoomCheck-070c5">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/cb474ca9-1894-4b5c-8449-15a5fbe5138f.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-924a0" hidden>
<label for="zoomCheck-924a0">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/f525cff1-96d2-4d7e-9776-dbd2e87bb58a.png#center" alt=""  />
</label></p>
<h3 id="research-questions">Research questions<a hidden class="anchor" aria-hidden="true" href="#research-questions">#</a></h3>
<ul>
<li>Is Quantize by channel is efficient?</li>
</ul>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<ul>
<li>Authors quantize by Input channels in order to isolete outliers. Outliers not appear everywhere, it only presistent in fixed channels. If we generally quantize channels outlier make affect all input channels.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-87151" hidden>
<label for="zoomCheck-87151">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/945fb346-da11-4f03-8f99-1fdeba8ac071.png#center" alt=""  />
</label></p>
<ul>
<li>ADAPTIVE PER-CHANNEL QUANTIZATION. as mentioned in Motivation not all layer is sensitive to Input channel and not all layer got huge magnituted named outlier. Authors propose determine oc and ic which method is more suit able for layer.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-e7b30" hidden>
<label for="zoomCheck-e7b30">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/a5086c6f-b25f-4007-ae90-8992beb2d260.png#center" alt=""  />
</label></p>
<h3 id="main-result">Main result<a hidden class="anchor" aria-hidden="true" href="#main-result">#</a></h3>
<ul>
<li>reorder option is make quantization make more good</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-2ccc7" hidden>
<label for="zoomCheck-2ccc7">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/4a15ab1b-b794-4538-a527-2e2be1fd30f5.png#center" alt=""  />
</label></p>
<ul>
<li>overall performance on quantization method base model is LLama</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-89e61" hidden>
<label for="zoomCheck-89e61">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/c5066a91-0fb0-4053-b535-94d0a8fdee0d.png#center" alt=""  />
</label></p>
<ul>
<li>accuracy on instructio-tuned LLama</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-1c662" hidden>
<label for="zoomCheck-1c662">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/646ca0de-b2a0-476f-a51e-d10442966dc6.png#center" alt=""  />
</label></p>
<ul>
<li>accuracy on Task-specific qunatization. target mean use calibrated dataset based on specific tast</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-a0ac7" hidden>
<label for="zoomCheck-a0ac7">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/336c0c03-dea0-40dc-ac61-6bd3839f91c9.png#center" alt=""  />
</label></p>
<ul>
<li>Perplexity based on method, group, bit. Using ada is make perplexity better</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-6e297" hidden>
<label for="zoomCheck-6e297">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/27d22d5a-6ca5-4750-b936-70c5505d0e10.png#center" alt=""  />
</label></p>
<ul>
<li>Recinstruction error per index. Initial layers more sensitive to outliers but All layers got different sensitivity based on task, model size, model type</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-4d142" hidden>
<label for="zoomCheck-4d142">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/055e0616-39dd-4549-96d5-04af5aaa9d0f.png#center" alt=""  />
</label></p>
<h3 id="personal-thought">Personal thought<a hidden class="anchor" aria-hidden="true" href="#personal-thought">#</a></h3>
<ul>
<li>Layers are got different sensitive based on task, model size. Is it mean outliers appear different channel or not appear based on task, size ?</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://new-sunset-shimmer.github.io/tags/quantization/">Quantization</a></li>
      <li><a href="https://new-sunset-shimmer.github.io/tags/paper/">Paper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://new-sunset-shimmer.github.io/">Sunset</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
