<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>QA-LORA | Sunset</title>
<meta name="keywords" content="Quantization, Paper">
<meta name="description" content="양자화된 Lora">
<meta name="author" content="">
<link rel="canonical" href="https://new-sunset-shimmer.github.io/QA-LORA/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://new-sunset-shimmer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://new-sunset-shimmer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://new-sunset-shimmer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://new-sunset-shimmer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://new-sunset-shimmer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://new-sunset-shimmer.github.io/QA-LORA/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><style>
    @media screen and (min-width: 769px) {

         
        .post-content input[type="checkbox"]:checked~label>img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style><meta property="og:title" content="QA-LORA" />
<meta property="og:description" content="양자화된 Lora" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://new-sunset-shimmer.github.io/QA-LORA/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="QA-LORA"/>
<meta name="twitter:description" content="양자화된 Lora"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://new-sunset-shimmer.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "QA-LORA",
      "item": "https://new-sunset-shimmer.github.io/QA-LORA/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "QA-LORA",
  "name": "QA-LORA",
  "description": "양자화된 Lora",
  "keywords": [
    "Quantization", "Paper"
  ],
  "articleBody": "2309.14717 (arxiv.org)\nQA-LORA: QUANTIZATION-AWARE LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\nReleased Date : 2023 10 09\nReview Date : 2024 10 19\n양자화된 Lora\nMotivation LLM의 높은 성능에도 불구하고 많은 계산비용과 높은 사이즈는 큰 걸림돌이 된다. 추가로 미세조정도 그 학습 데이터 크기에 비해 압도적으로 큰 모델은 종종 그 비용을 높게 만드는 원인중 하나가 된다. 그렇기에 양자화, Parameter-efficient fine-tunening(peft)가 많이 사용된다. 이번 논문에서는 모델 양자화와 양자화된 Lora를 적용시켜 두 문제를 해결할려고 한다. Methodology 해당 모델에서는 GPTQ를 사용하여 PTQ를 시행한다. QLoRA를 코드를 기준으로 작성하였다. 이번 논문에서는 GPTQ가 FP16이기에 양자화된 가중치에 더하기 위해서 PTQ가 변환하는 과정자체가 성능, 속도 측면에서 손실을 만드다고 서술했다. 그렇기에 LORA자체도 양자화하여 속도 측면에서 높은 우월성을 가지게 했다. 하지만 NF4로 양자화시 해당 타입을 위한 최적화 된 수식들이 없고 동시에 낮은 비트에서 성능이 굉장히 낮다는 단점이 있다. 저자들은 속도와 성능 그리고 사이즈 압축 측면을 해결하기 위해서 그룹 양자화라는 방법을 제안했다. Dout x K 의 Dout을 여러 그룹으로 쪼개 양자화를 하는 방식이다. Dout x K Main result 모델은 LLama와 LLaMA2를 사용. MMLU 벤치 마크에서의 zero-shot이랑 few-shot에서의 성능을 확인했으며 동시에 여러 과학 벤치마크도 사용했다. common sense에서의 reasoning ability도 확인.\nQLORA랑 같은 세팅을 했다. 미세조정은 FLAN v2, Alpaca를 사용했다.\n",
  "wordCount" : "185",
  "inLanguage": "en",
  "datePublished": "2024-12-12T00:00:00Z",
  "dateModified": "2024-12-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://new-sunset-shimmer.github.io/QA-LORA/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunset",
    "logo": {
      "@type": "ImageObject",
      "url": "https://new-sunset-shimmer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://new-sunset-shimmer.github.io/" accesskey="h" title="Sunset (Alt + H)">Sunset</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://new-sunset-shimmer.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      QA-LORA
    </h1>
    <div class="post-meta"><span title='2024-12-12 00:00:00 +0000 UTC'>December 12, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a></li>
                <li>
                    <a href="#main-result" aria-label="Main result">Main result</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><a href="https://arxiv.org/pdf/2309.14717">2309.14717 (arxiv.org)</a></p>
<blockquote>
<p>QA-LORA: QUANTIZATION-AWARE LOW-RANK
ADAPTATION OF LARGE LANGUAGE MODELS</p>
</blockquote>
<blockquote>
<p>Released Date : 2023 10 09</p>
</blockquote>
<blockquote>
<p>Review Date : 2024 10 19</p>
</blockquote>
<p>양자화된 Lora</p>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>LLM의 높은 성능에도 불구하고 많은 계산비용과 높은 사이즈는 큰 걸림돌이 된다. 추가로 미세조정도 그 학습 데이터 크기에 비해 압도적으로 큰 모델은 종종 그 비용을 높게 만드는 원인중 하나가 된다. 그렇기에 양자화, Parameter-efficient fine-tunening(peft)가 많이 사용된다. 이번 논문에서는 모델 양자화와 양자화된 Lora를 적용시켜 두 문제를 해결할려고 한다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-102aa" hidden>
<label for="zoomCheck-102aa">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80d4-9db6-c16e0d0744b5.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-eae3a" hidden>
<label for="zoomCheck-eae3a">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8063-8b96-c0cb60945bc6.png#center" alt=""  />
</label></p>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<ul>
<li>해당 모델에서는 GPTQ를 사용하여 PTQ를 시행한다. QLoRA를 코드를 기준으로  작성하였다. 이번 논문에서는 GPTQ가 FP16이기에 양자화된 가중치에 더하기 위해서 PTQ가 변환하는 과정자체가 성능, 속도 측면에서 손실을 만드다고 서술했다. 그렇기에 LORA자체도 양자화하여 속도 측면에서 높은 우월성을 가지게 했다. 하지만 NF4로 양자화시 해당 타입을 위한 최적화 된 수식들이 없고 동시에 낮은 비트에서 성능이 굉장히 낮다는 단점이 있다. 저자들은 속도와 성능 그리고 사이즈 압축 측면을 해결하기 위해서 그룹 양자화라는 방법을 제안했다. Dout x K 의 Dout을 여러 그룹으로 쪼개 양자화를 하는 방식이다. Dout x K</li>
</ul>
<h3 id="main-result">Main result<a hidden class="anchor" aria-hidden="true" href="#main-result">#</a></h3>
<ul>
<li>
<p>모델은 LLama와 LLaMA2를 사용. MMLU 벤치 마크에서의 zero-shot이랑 few-shot에서의 성능을 확인했으며 동시에 여러 과학 벤치마크도 사용했다. common sense에서의 reasoning ability도 확인.</p>
</li>
<li>
<p>QLORA랑 같은 세팅을 했다. 미세조정은 FLAN v2, Alpaca를 사용했다.</p>
</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-cf179" hidden>
<label for="zoomCheck-cf179">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8084-ad2a-d58401b3fbb5.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-b3d88" hidden>
<label for="zoomCheck-b3d88">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8034-aca3-f5c6702ab44b.png#center" alt=""  />
</label></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://new-sunset-shimmer.github.io/tags/quantization/">Quantization</a></li>
      <li><a href="https://new-sunset-shimmer.github.io/tags/paper/">Paper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://new-sunset-shimmer.github.io/">Sunset</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
