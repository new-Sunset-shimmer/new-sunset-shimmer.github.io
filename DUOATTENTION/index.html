<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DUOATTENTION | Sunset</title>
<meta name="keywords" content="Paper, Pruning">
<meta name="description" content="Attention sink &#43; Retrieval attention을 이용한 Token pruning">
<meta name="author" content="">
<link rel="canonical" href="https://new-sunset-shimmer.github.io/DUOATTENTION/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://new-sunset-shimmer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://new-sunset-shimmer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://new-sunset-shimmer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://new-sunset-shimmer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://new-sunset-shimmer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://new-sunset-shimmer.github.io/DUOATTENTION/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><style>
    @media screen and (min-width: 769px) {

         
        .post-content input[type="checkbox"]:checked~label>img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style><meta property="og:title" content="DUOATTENTION" />
<meta property="og:description" content="Attention sink &#43; Retrieval attention을 이용한 Token pruning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://new-sunset-shimmer.github.io/DUOATTENTION/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="DUOATTENTION"/>
<meta name="twitter:description" content="Attention sink &#43; Retrieval attention을 이용한 Token pruning"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://new-sunset-shimmer.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DUOATTENTION",
      "item": "https://new-sunset-shimmer.github.io/DUOATTENTION/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DUOATTENTION",
  "name": "DUOATTENTION",
  "description": "Attention sink + Retrieval attention을 이용한 Token pruning",
  "keywords": [
    "Paper", "Pruning"
  ],
  "articleBody": " Full Title : DUOATTENTION: EFFICIENT LONG-CONTEXT LLM INFERENCE WITH RETRIEVAL AND STREAMING HEADS\nLink : 2405.16406 (arxiv.org)\nRelated Link : 2404.15574 (arxiv.org), 2309.17453 (arxiv.org)\nReleased Date : 2024 10 14\nReview Date : 2024 11 03\nAttention sink + Retrieval attention을 이용한 Token pruning\nMotivation 현재 많은 LLM은 O(n^2d) 복잡도 문제를 겪고있다. 해당 문제를 해결하기 위해서 Attention head의 Key하고 Value를 재사용하는 방법을 사용하고있지만 cache 사이즈가 크다는 문제를 겪고있다. 이번 논문에서는 Attention Head 2를 써서 해당 문제를 해결할려고 한다. Research questions 효과적인 Token Pruning 방법은 존재하는가?\n모든 Head가 중요하지 않다면 해당 Head를 다른 방식으로 써도 되지 않을까? 2404.15574 (arxiv.org)\nMethodology Retrieval Head Mechanistically Explains Long-Context Factuality에서 가중치, activation, layer처럼 모든 head가 중요하지 않다는걸 실험으로 확인했다.\n저의 생각으로 저자들은 해당 결과를 기준으로 아이디어를 Token Prunning을하여 KV Cache를 줄이는 방식으로 발전시켰다.\n모든 head가 중요하지 않다는건 몇몇 head들만 Full attention으로 쓰고 나머지는 Approximate attention을 활용할수있다. 저자들은 해당 head를 2309.17453 (arxiv.org)에서 방법론을 가져왔으며 이를 Streaming head라고 명명했다.\n저자들은 2309.17453 (arxiv.org)에서 제안한 Attention sink + window attention을 Streaming Attetion명명했다. 실험적으로 Streaming head는 Streaming Attention의 영향을 즉 Token Pruning을 해도 성능에 큰 영향이 안간다. 이는 Streaming Head를 써도 큰 문제가 없음을 의미한다. 저자들은 학습 파라미터 Alpha를 둬서 Full Attention x alpha, Streaming Attention x (1 - alpha)를 둬서 학습한다. 이휴 서비스(Deployment)에 Alpha를 Threshold Tau로 어느 Head를 쓸지 정한다. 정의 : retrieval head : 최근 토큰과 attention sinks만으로 사용했을 때 모델 성능에 크게 영향을 끼치는 head. 저자들은 단순한 Natural language만으로 위의 학습이 어렵다고 판단했다. Natural language만으로는 head의 Retrieval 성능을 확인하기 어렵기에 실제 무수이 많은 Token에서 정답 일정한 길이에 Passkey만 찾는 dataset을 만들었다. 실제 학습단계에서는 attention score가 위의 식처럼 계산이 된다. M casual은 일반 attention 마스크이며 M streaming은 Λ형태의 마스크다.(오직 Attention sink와 recent token들만 보이게 하는 마스크). alpha는 0,1사의 값으로 초기화된다.\n저자들은 Loss function 2개를 만들었다. L_distill, L_reg. L_distill은 passkey의 token들의 attention값을 계산하여 L2 Norm을 한다. L_reg는 총 alpha값을 계산한다. 개인적인 생각으로 L_distill은 H_full과 H_mixed의 Attention값을 0으로 만드는 방향으로 학습된다고 생각한다. 그렇기에 L2 norm을 사용했고 따로 Lr을 적용시키지 않은것이다. L_reg은 H_Full의 중요도를 계산하는 의도로 보인다. 이번 논문의 L_reg을 만드는데 영감을 준 cnn filter pruning에서 채널의 중요도를 계산한다. 여기서는 H Full의 중요도를 적게 만드는 방향으로 학습한다. 최종 loss는 Retrieval 해드를 적게쓰면서도 방향으로 학습하면서 L_dsitill을 완충제로 사용하여 모든 해드가 Streaming이 되지 않도록한다. 해당 논문에서는 FlashAttention-2를 사용한다. 해당 방법론특성상 Context길이가 짧으면 짧을수록 속도가 일반 Attention보다 빠르게 올라간다. 그렇기에 Context를 일정한 길이의 길이로 나눈다. 이를 Chunked pre-filling이다. 이를 통해서 복잡도가 O(L^2)에서 O(LK)로 줄어든다. 도한 메모리 복잡도가 O(L)에서 O(K)로 줄어든다. Main result needle in stack 성능 long bench의 성능 short bench에서의 성능 Chunked prefill이 속도와 메모리 측면에서 굉장히 효율적이다. 양자화와 같이 사용시 속도가 더 빨라진다. Personal thought Loss function을 보면 결국 최대한 Full attention을 사용하지 않는 방향으로 학습이된다. 비록 L_distill로 방충망을 만들었어도 결국 Retrieval head가 필요한 시점에서도 Streaming head를 쓰게 되지 않나? ",
  "wordCount" : "448",
  "inLanguage": "en",
  "datePublished": "2024-12-12T00:00:00Z",
  "dateModified": "2024-12-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://new-sunset-shimmer.github.io/DUOATTENTION/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunset",
    "logo": {
      "@type": "ImageObject",
      "url": "https://new-sunset-shimmer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://new-sunset-shimmer.github.io/" accesskey="h" title="Sunset (Alt + H)">Sunset</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://new-sunset-shimmer.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      DUOATTENTION
    </h1>
    <div class="post-meta"><span title='2024-12-12 00:00:00 +0000 UTC'>December 12, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#research-questions" aria-label="Research questions">Research questions</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a></li>
                <li>
                    <a href="#main-result" aria-label="Main result">Main result</a></li>
                <li>
                    <a href="#personal-thought" aria-label="Personal thought">Personal thought</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Full Title : DUOATTENTION: EFFICIENT LONG-CONTEXT LLM INFERENCE WITH RETRIEVAL AND STREAMING HEADS</p>
</blockquote>
<blockquote>
<p>Link : <a href="https://arxiv.org/pdf/2405.16406">2405.16406 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Related Link : <a href="https://arxiv.org/pdf/2404.15574">2404.15574 (arxiv.org)</a>, <a href="https://arxiv.org/pdf/2309.17453">2309.17453 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Released Date : 2024 10 14</p>
</blockquote>
<blockquote>
<p>Review Date : 2024 11 03</p>
</blockquote>
<p>Attention sink + Retrieval attention을 이용한 Token pruning</p>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>현재 많은 LLM은 O(n^2d) 복잡도 문제를 겪고있다. 해당 문제를 해결하기 위해서 Attention head의 Key하고 Value를 재사용하는 방법을 사용하고있지만 cache 사이즈가 크다는 문제를 겪고있다. 이번 논문에서는 Attention Head 2를 써서 해당 문제를 해결할려고 한다.</li>
</ul>
<h3 id="research-questions">Research questions<a hidden class="anchor" aria-hidden="true" href="#research-questions">#</a></h3>
<ul>
<li>
<p>효과적인 Token Pruning 방법은 존재하는가?</p>
</li>
<li>
<p>모든 Head가 중요하지 않다면 해당 Head를 다른 방식으로 써도 되지 않을까? <a href="https://arxiv.org/pdf/2404.15574">2404.15574 (arxiv.org)</a></p>
</li>
</ul>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<p>

<input type="checkbox" id="zoomCheck-15840" hidden>
<label for="zoomCheck-15840">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80ea-bcdc-e3ac07be69cd.png#center" alt=""  />
</label></p>
<ul>
<li>
<p>Retrieval Head Mechanistically Explains Long-Context Factuality에서 가중치, activation, layer처럼 모든 head가 중요하지 않다는걸 실험으로 확인했다.</p>
</li>
<li>
<p>저의 생각으로 저자들은 해당 결과를 기준으로 아이디어를 Token Prunning을하여 KV Cache를 줄이는 방식으로 발전시켰다.</p>
</li>
<li>
<p>모든 head가 중요하지 않다는건 몇몇 head들만 Full attention으로 쓰고 나머지는 Approximate attention을 활용할수있다. 저자들은 해당 head를 <a href="https://arxiv.org/pdf/2309.17453">2309.17453 (arxiv.org)</a>에서 방법론을 가져왔으며 이를 Streaming head라고 명명했다.</p>
</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-f5e58" hidden>
<label for="zoomCheck-f5e58">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8098-8992-d1e16871f76e.png#center" alt=""  />
</label></p>
<ul>
<li>저자들은 <a href="https://arxiv.org/pdf/2309.17453">2309.17453 (arxiv.org)</a>에서 제안한 Attention sink + window attention을 Streaming Attetion명명했다. 실험적으로 Streaming head는 Streaming Attention의 영향을 즉 Token Pruning을 해도 성능에 큰 영향이 안간다. 이는 Streaming Head를 써도 큰 문제가 없음을 의미한다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-9e437" hidden>
<label for="zoomCheck-9e437">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8000-83ee-fee954be628a.png#center" alt=""  />
</label></p>
<ul>
<li>저자들은 학습 파라미터 Alpha를 둬서 Full Attention x alpha, Streaming Attention x (1 - alpha)를 둬서 학습한다. 이휴 서비스(Deployment)에 Alpha를 Threshold Tau로 어느 Head를 쓸지 정한다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-4483a" hidden>
<label for="zoomCheck-4483a">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/764ed612-dcf0-49d6-9ae6-ca35e04373b9.png#center" alt=""  />
</label></p>
<ul>
<li>정의 : retrieval head : 최근 토큰과 attention sinks만으로 사용했을 때 모델 성능에 크게 영향을 끼치는 head.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-4136b" hidden>
<label for="zoomCheck-4136b">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8090-b64b-fd3cf9dc606d.png#center" alt=""  />
</label></p>
<ul>
<li>저자들은 단순한 Natural language만으로 위의 학습이 어렵다고 판단했다. Natural language만으로는 head의 Retrieval 성능을 확인하기 어렵기에 실제 무수이 많은 Token에서 정답 일정한 길이에 Passkey만 찾는 dataset을 만들었다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-a7a69" hidden>
<label for="zoomCheck-a7a69">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8003-8851-e013b47894dc.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-ae544" hidden>
<label for="zoomCheck-ae544">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-800d-ace8-e40b94367c50.png#center" alt=""  />
</label></p>
<p>실제 학습단계에서는 attention score가 위의 식처럼 계산이 된다. M casual은 일반 attention 마스크이며 M streaming은 Λ형태의 마스크다.(오직 Attention sink와 recent token들만 보이게 하는 마스크). alpha는 0,1사의 값으로 초기화된다.</p>
<p>

<input type="checkbox" id="zoomCheck-aa94d" hidden>
<label for="zoomCheck-aa94d">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80b1-b9f3-ceae3591ce70.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-81d8f" hidden>
<label for="zoomCheck-81d8f">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-802c-b6a3-f68b91bf2730.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-56573" hidden>
<label for="zoomCheck-56573">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8011-8a62-d88427467296.png#center" alt=""  />
</label></p>
<ul>
<li>저자들은 Loss function 2개를 만들었다. L_distill, L_reg. L_distill은 passkey의 token들의 attention값을 계산하여 L2 Norm을 한다. L_reg는 총 alpha값을 계산한다.
개인적인 생각으로 L_distill은 H_full과 H_mixed의 Attention값을 0으로 만드는 방향으로 학습된다고 생각한다. 그렇기에 L2 norm을 사용했고 따로 Lr을 적용시키지 않은것이다.
L_reg은 H_Full의 중요도를 계산하는 의도로 보인다. 이번 논문의 L_reg을 만드는데 영감을 준 cnn filter pruning에서 채널의 중요도를 계산한다. 여기서는 H Full의 중요도를 적게 만드는 방향으로 학습한다.
최종 loss는 Retrieval 해드를 적게쓰면서도 방향으로 학습하면서 L_dsitill을 완충제로 사용하여 모든 해드가 Streaming이 되지 않도록한다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-69526" hidden>
<label for="zoomCheck-69526">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80b0-bb10-f8dae9d07bc2.png#center" alt=""  />
</label></p>
<ul>
<li>해당 논문에서는 FlashAttention-2를 사용한다. 해당 방법론특성상 Context길이가 짧으면 짧을수록 속도가 일반 Attention보다 빠르게 올라간다. 그렇기에 Context를 일정한 길이의 길이로 나눈다. 이를 Chunked pre-filling이다. 이를 통해서 복잡도가 O(L^2)에서 O(LK)로 줄어든다. 도한 메모리 복잡도가 O(L)에서 O(K)로 줄어든다.</li>
</ul>
<h3 id="main-result">Main result<a hidden class="anchor" aria-hidden="true" href="#main-result">#</a></h3>
<ul>
<li>needle in stack 성능</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-c2ac1" hidden>
<label for="zoomCheck-c2ac1">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80ec-9e19-f2c9559d9a7f.png#center" alt=""  />
</label></p>
<ul>
<li>long bench의 성능</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-ae14d" hidden>
<label for="zoomCheck-ae14d">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8079-8a02-c522cc243cf9.png#center" alt=""  />
</label></p>
<ul>
<li>short bench에서의 성능</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-bd94b" hidden>
<label for="zoomCheck-bd94b">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80cb-995b-d27959f1584a.png#center" alt=""  />
</label></p>
<ul>
<li>Chunked prefill이 속도와 메모리 측면에서 굉장히 효율적이다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-ed127" hidden>
<label for="zoomCheck-ed127">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-806e-8052-f75221de279c.png#center" alt=""  />
</label></p>
<ul>
<li>양자화와 같이 사용시 속도가 더 빨라진다.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-c1277" hidden>
<label for="zoomCheck-c1277">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-807b-8324-f0da178ad0b3.png#center" alt=""  />
</label></p>
<h3 id="personal-thought">Personal thought<a hidden class="anchor" aria-hidden="true" href="#personal-thought">#</a></h3>
<ol>
<li>Loss function을 보면 결국 최대한 Full attention을 사용하지 않는 방향으로 학습이된다. 비록 L_distill로 방충망을 만들었어도 결국 Retrieval head가 필요한 시점에서도 Streaming head를 쓰게 되지 않나?</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://new-sunset-shimmer.github.io/tags/paper/">Paper</a></li>
      <li><a href="https://new-sunset-shimmer.github.io/tags/pruning/">Pruning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://new-sunset-shimmer.github.io/">Sunset</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
