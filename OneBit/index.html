<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>OneBit | Sunset</title>
<meta name="keywords" content="Quantization, Paper">
<meta name="description" content="make one-bit model more cheaper way">
<meta name="author" content="">
<link rel="canonical" href="https://new-sunset-shimmer.github.io/OneBit/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://new-sunset-shimmer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://new-sunset-shimmer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://new-sunset-shimmer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://new-sunset-shimmer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://new-sunset-shimmer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://new-sunset-shimmer.github.io/OneBit/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><style>
    @media screen and (min-width: 769px) {

         
        .post-content input[type="checkbox"]:checked~label>img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style><meta property="og:title" content="OneBit" />
<meta property="og:description" content="make one-bit model more cheaper way" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://new-sunset-shimmer.github.io/OneBit/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-14T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="OneBit"/>
<meta name="twitter:description" content="make one-bit model more cheaper way"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://new-sunset-shimmer.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "OneBit",
      "item": "https://new-sunset-shimmer.github.io/OneBit/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "OneBit",
  "name": "OneBit",
  "description": "make one-bit model more cheaper way",
  "keywords": [
    "Quantization", "Paper"
  ],
  "articleBody": " Full Title : OneBit: Towards Extremely Low-bit Large Language Models\nLink : 2402.11295 (arxiv.org)\nRelated Link : https://github.com/xuyuzhuang11/OneBit, 2310.11453 (arxiv.org)\nReleased Date : Sat, 17 Feb 2024\nReview Date : Fri Dec 13 2024\nmake one-bit model more cheaper way\nMotivation To making model more faster and relax memory-bound 1-bit is effective way to quantize model. But most of the research quantize the model to 2-bit. In this paper quantize model to 1 bit but NOT TRAINING FROM SCRATCH. Research questions Can we quantize 1 bit model Methodology Background General quantization work under equation 1(RTN). But in 1 bit zero-point, scale parameter is useless\nFor quantize in 1-bit most recent work propose equation 2. But in that paper model can’t quantize from pre-trained model, we must train from scratch\n1-bit Linear Layer Architecture From quantize existing model authors propose equation 3. g and h are FP16 parameter. Even if they are extra parameter in FP16, they are in low bit also negligible. when author quantize 4096x4096 matrix total bit is 1.0073\nSign-Value-Independent Decomposition For future training authors initialize weight for efficient training. SVID(SINGULAR VECTOR INDEPENDENT DECOMPOSITION) is Decompose W to W_sign ⊙ W_value [W_sign = Sign(W), W_value = |W|]. We can decompose W to rank-1 approximation and can get equation 4\nIf we denote h and g as b, a moreover W_sing as W_+-1 we can get equation 5. this equation also dont need a restore original weight from Wsing and ab because x is can compute in decomposed matrices.\nthis Initialization of equation 3 is most near to the original linear equation(if we also consider a sign value).\nKnowledge Transfer In this paper transfer teacher model(Full precision model) to student model(1 bit model using equation 3). They use cross entropy loss but not in LM. They train g, h, and W in equation W\nIn equation 7 c is number of class in model and n is total sample(Tokens). Log is wrap student model.\nThey also use hidden state to Transfer knowlegde. n_l is layers and n_s training sample Hence the final objective function can be formulated as equation 9, alpha is hyper-parameter that balances the importance of the CE and MSE. in this paper use 1.0 Zero point quantization? Zero point, ensuring that the quantized representation can handle negative values or values centered around zero.\nNMF? Non-negative matrix factorization(NMF)\nEnsure that decomposed matrices have no negative values(also non-negative matrix approximation).\n[NMF — scikit-learn 1.5.2 documentation]\nFind two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) whose product approximates the non-negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\nL-2 norm L-2 norm is also known as Euclidean norm can measure distance. in using in MSE they are measure distance between two matrix and minimize it. Additionally, They are can use as penalty in loss function for prevalent overfitting and encourage smaller vector. from optimization aspect it tell us gradient size that in step.\nMain result Weight only, use NMF is scikit-learn, use same method for train data from LLM-QAT(2048 token) start from . 50 epoch, Adam, cosine scheduler. GPU is A100(!!!!). Author use Post-LayerNorm for stable qat because floating-point overflow during the QAT process Problem Solving Ability Efficiency T-L for each sign matrix Personal thought Can we train without W in equation 3 ?\nCan we BIT PTQ ?\n",
  "wordCount" : "562",
  "inLanguage": "en",
  "datePublished": "2024-12-14T00:00:00Z",
  "dateModified": "2024-12-14T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://new-sunset-shimmer.github.io/OneBit/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunset",
    "logo": {
      "@type": "ImageObject",
      "url": "https://new-sunset-shimmer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://new-sunset-shimmer.github.io/" accesskey="h" title="Sunset (Alt + H)">Sunset</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://new-sunset-shimmer.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      OneBit
    </h1>
    <div class="post-meta"><span title='2024-12-14 00:00:00 +0000 UTC'>December 14, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#research-questions" aria-label="Research questions">Research questions</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a></li>
                <li>
                    <a href="#main-result" aria-label="Main result">Main result</a></li>
                <li>
                    <a href="#personal-thought" aria-label="Personal thought">Personal thought</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Full Title : OneBit: Towards Extremely Low-bit Large Language Models</p>
</blockquote>
<blockquote>
<p>Link : <a href="https://arxiv.org/pdf/2402.11295">2402.11295 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Related Link : <a href="https://github.com/xuyuzhuang11/OneBit">https://github.com/xuyuzhuang11/OneBit</a>, <a href="https://arxiv.org/pdf/2310.11453">2310.11453 (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Released Date :  Sat, 17 Feb 2024</p>
</blockquote>
<blockquote>
<p>Review Date : Fri Dec 13 2024</p>
</blockquote>
<p>make one-bit model more cheaper way</p>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>To making model more faster and relax memory-bound 1-bit is effective way to quantize model. But most of the research quantize the model to 2-bit. In this paper quantize model to 1 bit but NOT TRAINING FROM SCRATCH.</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-ebddd" hidden>
<label for="zoomCheck-ebddd">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/6cf5ae8e-7556-4e63-b22f-dde82f80e8f7.png#center" alt=""  />
</label></p>
<h3 id="research-questions">Research questions<a hidden class="anchor" aria-hidden="true" href="#research-questions">#</a></h3>
<ul>
<li>Can we quantize 1 bit model</li>
</ul>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<ul>
<li>Background</li>
</ul>
<p>General quantization work under equation 1(RTN). But in 1 bit zero-point, scale parameter is useless</p>
<p>

<input type="checkbox" id="zoomCheck-d0730" hidden>
<label for="zoomCheck-d0730">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/1db2c6ea-72df-48d5-81ef-be941d19be1f.png#center" alt=""  />
</label></p>
<p>For quantize in 1-bit most recent work propose equation 2. But in that paper model can’t quantize from pre-trained model, we must train from scratch</p>
<p>

<input type="checkbox" id="zoomCheck-bc8c3" hidden>
<label for="zoomCheck-bc8c3">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15c415f9-e659-8049-a108-cad0756a4f78.png#center" alt=""  />
</label></p>
<ul>
<li>1-bit Linear Layer Architecture</li>
</ul>
<p>From quantize existing model authors propose equation 3. g and h are FP16 parameter. Even if they are extra parameter in FP16, they are in low bit also negligible. when author quantize 4096x4096 matrix total bit is 1.0073</p>
<p>

<input type="checkbox" id="zoomCheck-5b23f" hidden>
<label for="zoomCheck-5b23f">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15c415f9-e659-809b-9e13-ef47a83e7643.png#center" alt=""  />
</label></p>
<ul>
<li>Sign-Value-Independent Decomposition</li>
</ul>
<p>For future training authors initialize weight for efficient training. SVID(SINGULAR VECTOR INDEPENDENT DECOMPOSITION) is Decompose W to W_sign ⊙ W_value [W_sign = Sign(W), W_value = |W|]. We can decompose W to rank-1 approximation and can get equation 4</p>
<p>

<input type="checkbox" id="zoomCheck-411f2" hidden>
<label for="zoomCheck-411f2">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15c415f9-e659-8044-98db-c14cfcc3d3ee.png#center" alt=""  />
</label></p>
<p>If we denote h and g as b, a moreover W_sing as W_+-1 we can get equation 5. this equation also dont need a restore original weight from Wsing and ab because x is can compute in decomposed matrices.</p>
<p>

<input type="checkbox" id="zoomCheck-555cd" hidden>
<label for="zoomCheck-555cd">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15c415f9-e659-80e7-8772-d41b391395e8.png#center" alt=""  />
</label></p>
<p>this Initialization of equation 3 is most near to the original linear equation(if we also consider a sign value).</p>
<p>

<input type="checkbox" id="zoomCheck-21b29" hidden>
<label for="zoomCheck-21b29">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15c415f9-e659-80af-a4fa-f19c2d35bc8d.png#center" alt=""  />
</label></p>
<ul>
<li>Knowledge Transfer</li>
</ul>
<p>In this paper transfer teacher model(Full precision model) to student model(1 bit model using equation 3). They use cross entropy loss but not in LM. They train g, h, and W in equation W</p>
<p>In equation 7 c is number of class in model and n is total sample(Tokens). Log is wrap student model.</p>
<p>

<input type="checkbox" id="zoomCheck-9cbd7" hidden>
<label for="zoomCheck-9cbd7">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/6a19bf78-f694-4643-be69-7765b1efacca.png#center" alt=""  />
</label></p>
<pre><code>They also use hidden state to Transfer knowlegde. n_l is layers and n_s training sample
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-6ef2e" hidden>
<label for="zoomCheck-6ef2e">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/a9712e5b-d150-4902-bdbd-a45901de96e0.png#center" alt=""  />
</label></p>
<pre><code>Hence the final objective function can be formulated as equation 9, alpha is hyper-parameter that balances the importance of the CE and MSE. in this paper use 1.0
</code></pre>
<p>

<input type="checkbox" id="zoomCheck-5e950" hidden>
<label for="zoomCheck-5e950">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/adc7123d-3ca2-4fc0-bda0-ea9c4b3cf004.png#center" alt=""  />
</label></p>
<ul>
<li>Zero point quantization?</li>
</ul>
<p>Zero point, ensuring that the quantized representation can handle negative values or values centered around zero.</p>
<ul>
<li>NMF?</li>
</ul>
<p><strong>Non-negative matrix factorization(NMF)</strong></p>
<p>Ensure that decomposed matrices have no negative values(also non-negative matrix approximation).</p>
<p>[<a href="https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.NMF.html">NMF — scikit-learn 1.5.2 documentation</a>]</p>
<p>Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) whose product approximates the non-negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.</p>
<ul>
<li>L-2 norm</li>
</ul>
<p>L-2 norm is also known as  Euclidean norm can measure distance. in using in MSE they are measure distance between two matrix and minimize it. Additionally, They are can use as penalty in loss function for prevalent overfitting and encourage smaller vector. from optimization aspect it tell us gradient size that in step.</p>
<h3 id="main-result">Main result<a hidden class="anchor" aria-hidden="true" href="#main-result">#</a></h3>
<ul>
<li>Weight only, use NMF is scikit-learn, use same method for train data from LLM-QAT(2048 token) start from <BOS>. 50 epoch, Adam, cosine scheduler. GPU is A100(!!!!). Author use Post-LayerNorm for stable qat because floating-point overflow during the QAT process</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-5f5c0" hidden>
<label for="zoomCheck-5f5c0">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/5efb92d5-72ca-4589-b6dd-95d6e590ce16.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-5fcdf" hidden>
<label for="zoomCheck-5fcdf">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/16ba9595-e612-4e16-9054-15735dac2493.png#center" alt=""  />
</label></p>
<ul>
<li>Problem Solving Ability</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-7dc6c" hidden>
<label for="zoomCheck-7dc6c">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/0e54e4bb-0506-41d5-998d-d31e948a6907.png#center" alt=""  />
</label></p>
<ul>
<li>Efficiency</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-52158" hidden>
<label for="zoomCheck-52158">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/b11433e8-9e03-47b7-ae9b-0cce8e19c797.png#center" alt=""  />
</label></p>
<ul>
<li>T-L for each sign matrix</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-7ea07" hidden>
<label for="zoomCheck-7ea07">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/1228cd69-1c05-4c93-9b5c-8f851777d79d.png#center" alt=""  />
</label></p>
<h3 id="personal-thought">Personal thought<a hidden class="anchor" aria-hidden="true" href="#personal-thought">#</a></h3>
<ul>
<li>
<p>Can we train without W in equation 3 ?</p>
</li>
<li>
<p>Can we BIT PTQ ?</p>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://new-sunset-shimmer.github.io/tags/quantization/">Quantization</a></li>
      <li><a href="https://new-sunset-shimmer.github.io/tags/paper/">Paper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://new-sunset-shimmer.github.io/">Sunset</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
