<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>KVQuant | Sunset</title>
<meta name="keywords" content="Quantization, Paper">
<meta name="description" content="Long context에서의 Quantization">
<meta name="author" content="">
<link rel="canonical" href="https://new-sunset-shimmer.github.io/KVQuant/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://new-sunset-shimmer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://new-sunset-shimmer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://new-sunset-shimmer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://new-sunset-shimmer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://new-sunset-shimmer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://new-sunset-shimmer.github.io/KVQuant/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><style>
    @media screen and (min-width: 769px) {

         
        .post-content input[type="checkbox"]:checked~label>img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style><meta property="og:title" content="KVQuant" />
<meta property="og:description" content="Long context에서의 Quantization" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://new-sunset-shimmer.github.io/KVQuant/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="KVQuant"/>
<meta name="twitter:description" content="Long context에서의 Quantization"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://new-sunset-shimmer.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "KVQuant",
      "item": "https://new-sunset-shimmer.github.io/KVQuant/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "KVQuant",
  "name": "KVQuant",
  "description": "Long context에서의 Quantization",
  "keywords": [
    "Quantization", "Paper"
  ],
  "articleBody": " Full Title : KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\nLink : KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization (arxiv.org)\nReleased Date : 2024 06 04\nReview Date : 2024 10 26\nLong context에서의 Quantazation\nMotivation LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 이번 논문에서는 KV cache의 K를 RoPE(Rotary Position Embedding) non-uniform quantazaiton을 하여 긴 context에도 효율적으로 계산할수있게 한다. 또한 Research questions Long context에서의 양자화가 가능할까? Methodology 해당 논문에서는 총 4개의 방법론을 적용시키고 있다.\nPer-Channel Key Quantazation : Key matrix를 Channel별로 양자화시 좋은 성능이 나온다는걸 확인했다(quary는 아님). 그 이유를 Key는 outlier가 잘 발생하지 않고 한 채널은 스케일링 값과 0값을 공유해서 그렇다고 저자들은 서술했다.\nPre-RoPE Key Quantization : Rope를 적용시킨채 Key를 양자화할 경우 한 매트릭스에 위치 정보 및 여러 정보가 섞이기에 분포도가 바뀔수있다. 이는 양자화를 어렵게 만들기에 RoPE를 적용시키기전에 양자화를 진행한다.\nNon-Uniform Quantazation : 등 양자화는 KV 캐시 양자화에 비효율적인데, 이는 Query와 Key 활성화 값이 비균등하게 분포되어 있기 때문입니다. 또한, KV 캐시 로딩은 배치 크기나 시퀀스 길이와 관계없이 메모리 대역폭에 의해 제한되므로, 비균등 양자화에서 발생하는 복원 오버헤드는 문제가 되지 않습니다(추가되는 연산이 지연을 초래하지 않음). 따라서 KV 캐시 양자화에는 비균등 양자화 방법을 활용하는 것이 바람직합니다. 비균등 데이터타입을 계산해야하는데 이는 온라인으로 하기 힘들기에 사전에 calibration dataset으로 offline에서 계산합니다.\n- Per-Vector Dense-and-Sparse Quantization : 벡터마다 다른 값을 의미를 가지고있기에 해당 벡터의 outlier가 꼭 다른 Layer에서 outlier가 된다는 보장이 없다. 그렇기에 벡터별로 다른 outlier Threshold를 둬 보다 동적으로 양자화를 한다. 이들은 16FP 그대로 저장된다. - Attention Sink-Aware Quantization : Sink-Aware는 양자화에도 영향을 미친다는걸 저자들은 알아냈다. 그렇기에 첫번째 token만 16FP로 나둔다. 앞서 언급한 Non-Uniform Quantazation에서 첫번째 Token은 없앤다. Main result Personal thought Activation 도 같이 quantazation이 가능한가?\n왜 FLashattention, pageattention을 안 썼는가?\n만약 context길이 늘어나면 어떨까?\n",
  "wordCount" : "283",
  "inLanguage": "en",
  "datePublished": "2024-12-12T00:00:00Z",
  "dateModified": "2024-12-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://new-sunset-shimmer.github.io/KVQuant/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunset",
    "logo": {
      "@type": "ImageObject",
      "url": "https://new-sunset-shimmer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://new-sunset-shimmer.github.io/" accesskey="h" title="Sunset (Alt + H)">Sunset</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://new-sunset-shimmer.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://new-sunset-shimmer.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      KVQuant
    </h1>
    <div class="post-meta"><span title='2024-12-12 00:00:00 +0000 UTC'>December 12, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#research-questions" aria-label="Research questions">Research questions</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a></li>
                <li>
                    <a href="#main-result" aria-label="Main result">Main result</a></li>
                <li>
                    <a href="#personal-thought" aria-label="Personal thought">Personal thought</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Full Title : KVQuant: Towards 10 Million Context Length LLM Inference
with KV Cache Quantization</p>
</blockquote>
<blockquote>
<p>Link : <a href="https://arxiv.org/pdf/2401.18079">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization (arxiv.org)</a></p>
</blockquote>
<blockquote>
<p>Released Date : 2024 06 04</p>
</blockquote>
<blockquote>
<p>Review Date : 2024 10 26</p>
</blockquote>
<p>Long context에서의 Quantazation</p>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>LLM의 놀라운 성능에도 불구하고 높은 계산량, 큰 사이즈등의 문제가 존재한다. 이번 논문에서는 KV cache의 K를 RoPE(Rotary Position Embedding) non-uniform quantazaiton을 하여 긴 context에도 효율적으로 계산할수있게 한다. 또한</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-ae344" hidden>
<label for="zoomCheck-ae344">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/a0f10b73-95ff-4179-b4ce-6600dc1abfe8.png#center" alt=""  />
</label></p>
<h3 id="research-questions">Research questions<a hidden class="anchor" aria-hidden="true" href="#research-questions">#</a></h3>
<ul>
<li>Long context에서의 양자화가 가능할까?</li>
</ul>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<ul>
<li>
<p>해당 논문에서는 총 4개의 방법론을 적용시키고 있다.</p>
<ul>
<li>
<p>Per-Channel Key Quantazation : Key matrix를 Channel별로 양자화시 좋은 성능이 나온다는걸 확인했다(quary는 아님). 그 이유를 Key는 outlier가 잘 발생하지 않고 한 채널은 스케일링 값과 0값을 공유해서 그렇다고 저자들은 서술했다.</p>
</li>
<li>
<p>Pre-RoPE Key Quantization : Rope를 적용시킨채 Key를 양자화할 경우 한 매트릭스에 위치 정보 및 여러 정보가 섞이기에 분포도가 바뀔수있다. 이는 양자화를 어렵게 만들기에 RoPE를 적용시키기전에 양자화를 진행한다.</p>
</li>
<li>
<p>Non-Uniform Quantazation : 등 양자화는 KV 캐시 양자화에 비효율적인데, 이는 Query와 Key 활성화 값이 비균등하게 분포되어 있기 때문입니다. 또한, KV 캐시 로딩은 배치 크기나 시퀀스 길이와 관계없이 메모리 대역폭에 의해 제한되므로, 비균등 양자화에서 발생하는 복원 오버헤드는 문제가 되지 않습니다(추가되는 연산이 지연을 초래하지 않음). 따라서 KV 캐시 양자화에는 비균등 양자화 방법을 활용하는 것이 바람직합니다. 비균등 데이터타입을 계산해야하는데 이는 온라인으로 하기 힘들기에 사전에 calibration dataset으로 offline에서 계산합니다.</p>
</li>
</ul>
</li>
</ul>
<p>

<input type="checkbox" id="zoomCheck-210dd" hidden>
<label for="zoomCheck-210dd">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/02fb5475-39b7-41c8-b4c7-1830f2f66b1c.png#center" alt=""  />
</label></p>
<pre><code>- Per-Vector Dense-and-Sparse Quantization : 벡터마다 다른 값을 의미를 가지고있기에 해당 벡터의 outlier가 꼭 다른 Layer에서 outlier가 된다는 보장이 없다. 그렇기에 벡터별로 다른 outlier Threshold를 둬 보다 동적으로 양자화를 한다. 이들은 16FP 그대로 저장된다.

- Attention Sink-Aware Quantization : Sink-Aware는 양자화에도 영향을 미친다는걸 저자들은 알아냈다. 그렇기에 첫번째 token만 16FP로 나둔다. 앞서 언급한 Non-Uniform Quantazation에서 첫번째 Token은 없앤다.
</code></pre>
<h3 id="main-result">Main result<a hidden class="anchor" aria-hidden="true" href="#main-result">#</a></h3>
<p>

<input type="checkbox" id="zoomCheck-8f34a" hidden>
<label for="zoomCheck-8f34a">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80eb-b6ee-c42baa52178c.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-d3a21" hidden>
<label for="zoomCheck-d3a21">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8057-8874-fc1f22878de2.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-f831c" hidden>
<label for="zoomCheck-f831c">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-80ce-b281-ef04995fb348.png#center" alt=""  />
</label></p>
<p>

<input type="checkbox" id="zoomCheck-48036" hidden>
<label for="zoomCheck-48036">
    <img class="zoomCheck" loading="lazy" decoding="async" src="/images/15a415f9-e659-8047-a6c5-fb3cd0960805.png#center" alt=""  />
</label></p>
<hr>
<h3 id="personal-thought">Personal thought<a hidden class="anchor" aria-hidden="true" href="#personal-thought">#</a></h3>
<ol>
<li>
<p>Activation 도 같이 quantazation이 가능한가?</p>
</li>
<li>
<p>왜 FLashattention, pageattention을 안 썼는가?</p>
</li>
<li>
<p>만약 context길이 늘어나면 어떨까?</p>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://new-sunset-shimmer.github.io/tags/quantization/">Quantization</a></li>
      <li><a href="https://new-sunset-shimmer.github.io/tags/paper/">Paper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://new-sunset-shimmer.github.io/">Sunset</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
